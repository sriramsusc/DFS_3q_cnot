diff --git a/QCRL_v0.2/ex_operations.py:Zone.Identifier b/QCRL_v0.2/ex_operations.py:Zone.Identifier
deleted file mode 100644
index e69de29..0000000
diff --git a/QCRL_v0.2/exch_gym_env.py:Zone.Identifier b/QCRL_v0.2/exch_gym_env.py:Zone.Identifier
deleted file mode 100644
index e69de29..0000000
diff --git a/QCRL_v0.2/fw_target.py:Zone.Identifier b/QCRL_v0.2/fw_target.py:Zone.Identifier
deleted file mode 100644
index e69de29..0000000
diff --git a/QCRL_v0.2/main.py:Zone.Identifier b/QCRL_v0.2/main.py:Zone.Identifier
deleted file mode 100644
index e69de29..0000000
diff --git a/QCRL_v0.2/pdqn_exchange_cnot/formatted_total_config.py b/QCRL_v0.2/pdqn_exchange_cnot/formatted_total_config.py
deleted file mode 100644
index dea1fef..0000000
--- a/QCRL_v0.2/pdqn_exchange_cnot/formatted_total_config.py
+++ /dev/null
@@ -1,138 +0,0 @@
-from easydict import EasyDict
-
-main_config = dict(
-    exp_name='pdqn_exchange_cnot',
-    env=dict(
-        manager=dict(
-            episode_num=float('inf'),
-            max_retry=1,
-            retry_type='reset',
-            auto_reset=True,
-            step_timeout=None,
-            reset_timeout=None,
-            retry_waiting_time=0.1,
-            cfg_type='BaseEnvManagerDict',
-            type='base',
-        ),
-        stop_value=10000000000,
-        n_evaluator_episode=4,
-        max_episode_steps=18,
-        collector_env_num=1,
-        evaluator_env_num=1,
-        use_act_scale=True,
-    ),
-    policy=dict(
-        model=dict(
-            obs_shape=168,
-            action_shape={'action_type_shape': 5, 'action_args_shape': 1, 'encoder_hidden_size_list': [256, 256, 256]},
-        ),
-        learn=dict(
-            learner=dict(
-                train_iterations=1000000000,
-                dataloader=dict(
-                    num_workers=0,
-                ),
-                log_policy=True,
-                hook=dict(
-                    load_ckpt_before_run='',
-                    log_show_after_iter=100,
-                    save_ckpt_after_iter=10000,
-                    save_ckpt_after_run=True,
-                ),
-                cfg_type='BaseLearnerDict',
-            ),
-            resume_training=False,
-            update_per_collect=3,
-            batch_size=64,
-            learning_rate=0.001,
-            target_theta=0.005,
-            ignore_done=False,
-            multi_gpu=False,
-            hook={'load_on_driver': True},
-            train_epoch=100,
-            learning_rate_dis=0.001,
-            learning_rate_cont=0.001,
-            update_circle=10,
-            weight_decay=0,
-        ),
-        collect=dict(
-            collector=dict(
-                deepcopy_obs=False,
-                transform_obs=False,
-                collect_print_freq=100,
-                cfg_type='SampleSerialCollectorDict',
-                type='sample',
-            ),
-            unroll_len=1,
-            noise_sigma=0.7,
-            n_sample=320,
-            noise=True,
-        ),
-        eval=dict(
-            evaluator=dict(
-                eval_freq=10,
-                render={'render_freq': -1, 'mode': 'train_iter'},
-                figure_path=None,
-                cfg_type='InteractionSerialEvaluatorDict',
-                n_episode=4,
-                stop_value=10000000000,
-            ),
-        ),
-        other=dict(
-            replay_buffer=dict(
-                type='advanced',
-                replay_buffer_size=100000,
-                max_use=float('inf'),
-                max_staleness=float('inf'),
-                alpha=0.6,
-                beta=0.4,
-                anneal_step=100000,
-                enable_track_used_data=False,
-                deepcopy=False,
-                thruput_controller=dict(
-                    push_sample_rate_limit=dict(
-                        max=float('inf'),
-                        min=0,
-                    ),
-                    window_seconds=30,
-                    sample_min_limit_ratio=1,
-                ),
-                monitor=dict(
-                    sampled_data_attr=dict(
-                        average_range=5,
-                        print_freq=200,
-                    ),
-                    periodic_thruput=dict(
-                        seconds=60,
-                    ),
-                ),
-                cfg_type='AdvancedReplayBufferDict',
-            ),
-        ),
-        on_policy=False,
-        cuda=True,
-        multi_gpu=False,
-        bp_update_sync=True,
-        traj_len_inf=False,
-        priority=False,
-        priority_IS_weight=False,
-        discount_factor=0.97,
-        nstep=1,
-        cfg_type='PDQNCommandModePolicyDict',
-    ),
-)
-main_config = EasyDict(main_config)
-main_config = main_config
-create_config = dict(
-    env=dict(
-        import_names=['exch_gym_env'],
-        type='ExchangeCNOTEnvDI',
-    ),
-    env_manager=dict(
-        cfg_type='BaseEnvManagerDict',
-        type='base',
-    ),
-    policy=dict(type='pdqn'),
-)
-create_config = EasyDict(create_config)
-create_config = create_config
diff --git a/QCRL_v0.2/pdqn_exchange_cnot/log/buffer/buffer_logger.txt b/QCRL_v0.2/pdqn_exchange_cnot/log/buffer/buffer_logger.txt
deleted file mode 100644
index e69de29..0000000
diff --git a/QCRL_v0.2/pdqn_exchange_cnot/log/collector/collector_logger.txt b/QCRL_v0.2/pdqn_exchange_cnot/log/collector/collector_logger.txt
deleted file mode 100644
index e69de29..0000000
diff --git a/QCRL_v0.2/pdqn_exchange_cnot/log/evaluator/evaluator_logger.txt b/QCRL_v0.2/pdqn_exchange_cnot/log/evaluator/evaluator_logger.txt
deleted file mode 100644
index e69de29..0000000
diff --git a/QCRL_v0.2/pdqn_exchange_cnot/log/learner/learner_logger.txt b/QCRL_v0.2/pdqn_exchange_cnot/log/learner/learner_logger.txt
deleted file mode 100644
index e69de29..0000000
diff --git a/QCRL_v0.2/pdqn_exchange_cnot/log/serial/events.out.tfevents.1749465045.Kyoka-Suigetsu b/QCRL_v0.2/pdqn_exchange_cnot/log/serial/events.out.tfevents.1749465045.Kyoka-Suigetsu
deleted file mode 100644
index c4aa135..0000000
Binary files a/QCRL_v0.2/pdqn_exchange_cnot/log/serial/events.out.tfevents.1749465045.Kyoka-Suigetsu and /dev/null differ
diff --git a/QCRL_v0.2/pdqn_exchange_cnot/total_config.py b/QCRL_v0.2/pdqn_exchange_cnot/total_config.py
deleted file mode 100644
index 20d6611..0000000
--- a/QCRL_v0.2/pdqn_exchange_cnot/total_config.py
+++ /dev/null
@@ -1,143 +0,0 @@
-exp_config = {
-    'env': {
-        'manager': {
-            'episode_num': float("inf"),
-            'max_retry': 1,
-            'retry_type': 'reset',
-            'auto_reset': True,
-            'step_timeout': None,
-            'reset_timeout': None,
-            'retry_waiting_time': 0.1,
-            'cfg_type': 'BaseEnvManagerDict',
-            'type': 'base'
-        },
-        'stop_value': 10000000000,
-        'n_evaluator_episode': 4,
-        'import_names': ['exch_gym_env'],
-        'type': 'ExchangeCNOTEnvDI',
-        'max_episode_steps': 18,
-        'collector_env_num': 1,
-        'evaluator_env_num': 1,
-        'use_act_scale': True
-    },
-    'policy': {
-        'model': {
-            'obs_shape': 168,
-            'action_shape': {
-                'action_type_shape': 5,
-                'action_args_shape': 1,
-                'encoder_hidden_size_list': [256, 256, 256]
-            }
-        },
-        'learn': {
-            'learner': {
-                'train_iterations': 1000000000,
-                'dataloader': {
-                    'num_workers': 0
-                },
-                'log_policy': True,
-                'hook': {
-                    'load_ckpt_before_run': '',
-                    'log_show_after_iter': 100,
-                    'save_ckpt_after_iter': 10000,
-                    'save_ckpt_after_run': True
-                },
-                'cfg_type': 'BaseLearnerDict'
-            },
-            'resume_training': False,
-            'update_per_collect': 3,
-            'batch_size': 64,
-            'learning_rate': 0.001,
-            'target_theta': 0.005,
-            'ignore_done': False,
-            'multi_gpu': False,
-            'hook': {
-                'load_on_driver': True
-            },
-            'train_epoch': 100,
-            'learning_rate_dis': 0.001,
-            'learning_rate_cont': 0.001,
-            'update_circle': 10,
-            'weight_decay': 0
-        },
-        'collect': {
-            'collector': {
-                'deepcopy_obs': False,
-                'transform_obs': False,
-                'collect_print_freq': 100,
-                'cfg_type': 'SampleSerialCollectorDict',
-                'type': 'sample'
-            },
-            'unroll_len': 1,
-            'noise_sigma': 0.7,
-            'n_sample': 320,
-            'noise': True
-        },
-        'eval': {
-            'evaluator': {
-                'eval_freq': 10,
-                'render': {
-                    'render_freq': -1,
-                    'mode': 'train_iter'
-                },
-                'figure_path': None,
-                'cfg_type': 'InteractionSerialEvaluatorDict',
-                'n_episode': 4,
-                'stop_value': 10000000000
-            }
-        },
-        'other': {
-            'replay_buffer': {
-                'type': 'advanced',
-                'replay_buffer_size': 100000,
-                'max_use': float("inf"),
-                'max_staleness': float("inf"),
-                'alpha': 0.6,
-                'beta': 0.4,
-                'anneal_step': 100000,
-                'enable_track_used_data': False,
-                'deepcopy': False,
-                'thruput_controller': {
-                    'push_sample_rate_limit': {
-                        'max': float("inf"),
-                        'min': 0
-                    },
-                    'window_seconds': 30,
-                    'sample_min_limit_ratio': 1
-                },
-                'monitor': {
-                    'sampled_data_attr': {
-                        'average_range': 5,
-                        'print_freq': 200
-                    },
-                    'periodic_thruput': {
-                        'seconds': 60
-                    }
-                },
-                'cfg_type': 'AdvancedReplayBufferDict'
-            },
-            'eps': {
-                'type': 'exp',
-                'start': 1.0,
-                'end': 0.05,
-                'decay': 10000
-            },
-            'commander': {
-                'cfg_type': 'BaseSerialCommanderDict'
-            }
-        },
-        'on_policy': False,
-        'cuda': True,
-        'multi_gpu': False,
-        'bp_update_sync': True,
-        'traj_len_inf': False,
-        'type': 'pdqn_command',
-        'priority': False,
-        'priority_IS_weight': False,
-        'discount_factor': 0.97,
-        'nstep': 1,
-        'cfg_type': 'PDQNCommandModePolicyDict'
-    },
-    'exp_name': 'pdqn_exchange_cnot',
-    'seed': 42
-}
diff --git a/QCRL_v0.2/results.json b/QCRL_v0.2/results.json
deleted file mode 100644
index ca99cbf..0000000
--- a/QCRL_v0.2/results.json
+++ /dev/null
@@ -1,13 +0,0 @@
-{
-  "stop": "False",
-  "env_step": 25056,
-  "train_iter": 174,
-  "eval_value": -340.5,
-  "eval_value_raw": [
-    -340.5,
-    -340.5,
-    -340.5,
-    -340.5
-  ],
-  "finish_time": "Sun Jun  1 18:04:13 2025"
-}
\ No newline at end of file
diff --git a/QCRL_v0.2/results2.json b/QCRL_v0.2/results2.json
deleted file mode 100644
index 29bd9ea..0000000
--- a/QCRL_v0.2/results2.json
+++ /dev/null
@@ -1,13 +0,0 @@
-{
-  "stop": "False",
-  "env_step": 25056,
-  "train_iter": 174,
-  "eval_value": -340.5,
-  "eval_value_raw": [
-    -340.5,
-    -340.5,
-    -340.5,
-    -340.5
-  ],
-  "finish_time": "Sun Jun  1 19:39:25 2025"
-}
\ No newline at end of file
diff --git a/QCRL_v0.2/save_best_episode_hook.py b/QCRL_v0.2/save_best_episode_hook.py
deleted file mode 100644
index e69de29..0000000
diff --git a/QCRL_v0.2/verifier.ipynb:Zone.Identifier b/QCRL_v0.2/verifier.ipynb:Zone.Identifier
deleted file mode 100644
index e69de29..0000000
diff --git a/QCRL_v0.3/__pycache__/config.cpython-310.pyc b/QCRL_v0.3/__pycache__/config.cpython-310.pyc
deleted file mode 100644
index 08fe206..0000000
Binary files a/QCRL_v0.3/__pycache__/config.cpython-310.pyc and /dev/null differ
diff --git a/QCRL_v0.3/__pycache__/ex_operations.cpython-310.pyc b/QCRL_v0.3/__pycache__/ex_operations.cpython-310.pyc
deleted file mode 100644
index 544eca4..0000000
Binary files a/QCRL_v0.3/__pycache__/ex_operations.cpython-310.pyc and /dev/null differ
diff --git a/QCRL_v0.3/__pycache__/exch_gym_env.cpython-310.pyc b/QCRL_v0.3/__pycache__/exch_gym_env.cpython-310.pyc
deleted file mode 100644
index bb97529..0000000
Binary files a/QCRL_v0.3/__pycache__/exch_gym_env.cpython-310.pyc and /dev/null differ
diff --git a/QCRL_v0.3/__pycache__/fw_target.cpython-310.pyc b/QCRL_v0.3/__pycache__/fw_target.cpython-310.pyc
deleted file mode 100644
index eef8fc8..0000000
Binary files a/QCRL_v0.3/__pycache__/fw_target.cpython-310.pyc and /dev/null differ
diff --git a/QCRL_v0.3/action_utils.py b/QCRL_v0.3/action_utils.py
deleted file mode 100644
index 652d7bd..0000000
--- a/QCRL_v0.3/action_utils.py
+++ /dev/null
@@ -1,26 +0,0 @@
-import numpy as np
-from typing import Dict, Sequence
-
-MASK_LEN = 5  # number of neighbour pairs
-
-
-def dict_to_vec(action: Dict[str, float | Sequence[float]] | Dict) -> np.ndarray:
-    """Convert PDQN action dict to 6D vector."""
-    pair_idx = int(
-        action.get("action_type", action.get("action", action.get("action_index", 0)))
-    )
-    p_val = action.get("action_args", action.get("param", [0.0]))
-    p = float(p_val[0] if isinstance(p_val, (list, tuple, np.ndarray)) else p_val)
-    vec = np.zeros(MASK_LEN + 1, dtype=np.float32)
-    vec[pair_idx] = 1.0
-    vec[-1] = p
-    return vec
-
-
-def vec_to_dict(vec: Sequence[float]) -> Dict:
-    """Convert 6D vector back to PDQN action dict."""
-    if len(vec) != MASK_LEN + 1:
-        raise ValueError(f"Expected vector of length {MASK_LEN + 1}")
-    pair_idx = int(np.argmax(vec[:MASK_LEN]))
-    p = float(vec[-1])
-    return {"action_type": pair_idx, "action_args": [p]}
\ No newline at end of file
diff --git a/QCRL_v0.3/airl_reward.py b/QCRL_v0.3/airl_reward.py
deleted file mode 100644
index ea5ca9e..0000000
--- a/QCRL_v0.3/airl_reward.py
+++ /dev/null
@@ -1,50 +0,0 @@
-import torch
-import numpy as np
-from types import SimpleNamespace
-from inverse_rl.airl import AIRL
-from inverse_rl.discriminator_network import G, H
-
-
-class AIRLReward:
-    """Wrapper around the AIRL discriminator to provide rewards."""
-
-    def __init__(
-        self,
-        state_dim: int,
-        action_dim: int,
-        device: str = "cpu",
-        model_path: str | None = None,
-    ):
-        args = SimpleNamespace(
-            state_only=False,
-            layer_num=2,
-            hidden_dim=128,
-            activation_function=torch.relu,
-            last_activation=None,
-            gamma=0.99,
-            lr=1e-3,
-        )
-        self.model = AIRL(None, device, state_dim, action_dim, args)
-        self.model.to(device)
-        if model_path is not None:
-            self.model.load_state_dict(torch.load(model_path, map_location=device))
-        self.model.eval()
-        self.device = device
-
-    def __call__(
-        self,
-        log_prob: np.ndarray,
-        state: np.ndarray,
-        action: np.ndarray,
-        next_state: np.ndarray,
-        done: np.ndarray,
-    ) -> float:
-        with torch.no_grad():
-            r = self.model.get_reward(
-                torch.as_tensor(log_prob, dtype=torch.float32, device=self.device),
-                torch.as_tensor(state, dtype=torch.float32, device=self.device),
-                torch.as_tensor(action, dtype=torch.float32, device=self.device),
-                torch.as_tensor(next_state, dtype=torch.float32, device=self.device),
-                torch.as_tensor(done, dtype=torch.float32, device=self.device),
-            )
-            return float(r.cpu().numpy())
\ No newline at end of file
diff --git a/QCRL_v0.3/config.py b/QCRL_v0.3/config.py
deleted file mode 100644
index 8d6436d..0000000
--- a/QCRL_v0.3/config.py
+++ /dev/null
@@ -1,84 +0,0 @@
-from easydict import EasyDict as edict
-from ding.config import compile_config
-
-main_config = edict({
-    "exp_name": "pdqn_exchange_cnot",
-
-    
-    # ────────────────────── environment ────────────────────── #
-    "env": {
-        "import_names": ["exch_gym_env"],
-        "type": "ExchangeCNOTEnvDI",
-        "max_episode_steps": 18,
-        "collector_env_num": 1,
-        "evaluator_env_num": 1,
-        "use_act_scale": True,
-    },
-
-    # ───────────────────────── policy ───────────────────────── #
-    "policy": {
-        "type": "pdqn_command",
-        "cuda": True,  # use GPU for training
-        # ‣ model description → **one** dict for both branches
-        "model": {
-            "obs_shape": 168,
-            "action_shape": edict({
-                "action_type_shape": 5,   # discrete: 5 neighbour pairs
-                "action_args_shape": 1,   # continuous: swap-power p
-                "encoder_hidden_size_list": [256, 256, 256]
-            }),
-        },
-
-        # ‣ learning hyper-params
-        "learn": {
-            "multi_gpu": False,
-            "hook": {"load_on_driver": True},
-            "train_epoch": 100,
-            "batch_size": 64,
-
-            # ──► PDQN needs these two ◄──
-            "learning_rate_dis": 1e-3,   # discrete Q-network
-            "learning_rate_cont": 1e-3,  # continuous Q-network
-            "update_circle": 10,
-            "weight_decay": 0,
-            
-        },
-        # ‣ data collection / evaluation
-        "collect": {
-            "n_sample": 320,
-            # "unroll_len": 3,
-            "noise": True,
-            # NEW – Gaussian with σ=0.7 mapped to [-2,2]
-            "noise_sigma": 0.7,
-        },
-        "eval":    {"evaluator": {"eval_freq": 10, "n_episode": 5}},
-
-        # ‣ misc
-        "other": {
-            "eps": {
-                "type": "exp",
-                "start": 1.0,
-                "end": 0.05,
-                "decay": 10000,
-            },
-            "replay_buffer": {"replay_buffer_size": 100_000},
-        },
-    },
-})
-
-# create_cfg now includes the minimal pieces DI-engine needs
-create_config = edict({
-    # 1. env_manager key so compile_config won't crash
-    "env_manager": {
-        "type": "base",      # matches your main_config.manager
-    },
-    # 2. env must point to your registered class
-    "env": {
-        "import_names": ["exch_gym_env"],
-        "type": "ExchangeCNOTEnvDI",
-    },
-    # 3. policy command name
-    "policy": {
-        "type": "pdqn",
-    },
-})
\ No newline at end of file
diff --git a/QCRL_v0.3/ex_operations.py b/QCRL_v0.3/ex_operations.py
deleted file mode 100644
index 230469b..0000000
--- a/QCRL_v0.3/ex_operations.py
+++ /dev/null
@@ -1,47 +0,0 @@
-import jax, jax.numpy as jnp
-from functools import reduce
-
-from scipy.linalg import expm
-
-I = jnp.eye(2, dtype=complex)
-X = jnp.array([[0, 1], [1, 0]], dtype=complex)
-Y = jnp.array([[0, -1j], [1j, 0]], dtype=complex)
-Z = jnp.array([[1, 0], [0, -1]], dtype=complex)
-
-def exchange_generators(n,i,j):
-    def _kron_all(mats):
-        """Kronecker product of a list of matrices."""
-        return reduce(jnp.kron, mats)
-    
-    def two_qubit_term(pauli):
-        mats = [I] * n          # start with identities
-        mats[i] = mats[j] = pauli
-        return _kron_all(mats)
-
-    XX = two_qubit_term(X)
-    YY = two_qubit_term(Y)
-    ZZ = two_qubit_term(Z)
-    
-    return (XX + YY + ZZ) / 4.0  # Heisenberg exchange Hamiltonian
-     
-
-def exchange_gate_nqubits(n: int, p: float, i: int, j: int):
-    if not (0 <= i < n and 0 <= j < n and i != j):
-        raise ValueError("Indices i and j must be distinct and in [0 .. n-1].")   
-
-    def _kron_all(mats):
-        """Kronecker product of a list of matrices."""
-        return reduce(jnp.kron, mats)
-
-    def two_qubit_term(pauli):
-            mats = [I] * n          # start with identities
-            mats[i] = mats[j] = pauli
-            return _kron_all(mats)
-
-    XX = two_qubit_term(X)
-    YY = two_qubit_term(Y)
-    ZZ = two_qubit_term(Z)
-    H_swap_gen = (XX + YY + ZZ) / 4.0         # Heisenberg exchange Hamiltonian
-    U = expm(-1j * jnp.pi * p * H_swap_gen)   # e^(-i π p H_swap)
-
-    return U
\ No newline at end of file
diff --git a/QCRL_v0.3/exch_gym_env.py b/QCRL_v0.3/exch_gym_env.py
deleted file mode 100644
index 7024271..0000000
--- a/QCRL_v0.3/exch_gym_env.py
+++ /dev/null
@@ -1,381 +0,0 @@
-import gym
-from gym import spaces
-import math
-import numpy as np
-import jax, jax.numpy as jnp
-from jax import jit, vmap, lax
-from scipy.linalg import expm
-from ding.utils import ENV_REGISTRY
-from ex_operations import exchange_generators
-from fw_target import U_circuit as TARGET_FULL
-from copy import deepcopy
-from easydict import EasyDict
-from functools import partial
-from collections import namedtuple
-
-# ----------------------------------------------------------------------
-# 1. Static problem data
-# ----------------------------------------------------------------------
-N_PHYS = 6
-NEIGHBORS = jnp.array([[0, 1],
-                      [1, 2],
-                      [2, 3],
-                      [3, 4],
-                      [4, 5]], dtype=jnp.int32)
-LOGICAL = jnp.array([9,10,12,17,18,20,33,34,36], dtype=jnp.int32)
-TARGET_BLOCK = TARGET_FULL[LOGICAL][:, LOGICAL]
-# precompute generators
-H_BASE = jnp.stack(
-    [jnp.array(exchange_generators(N_PHYS, int(i), int(j))) for i,j in NEIGHBORS],
-    axis=0
-)
-# A minimal container with the fields DI-engine expects
-Timestep = namedtuple('Timestep', ['obs', 'reward', 'done', 'info'])
-
-INVALID_PENALTY = -20.0
-
-def _to_np(x, dtype=np.float32):          #  helper
-    return np.asarray(x, dtype=dtype)
-
-def _block(U: jnp.ndarray) -> jnp.ndarray:
-    return U[LOGICAL][:, LOGICAL]
-
-def _block_9x64(M):
-    """rows = LOGICAL, all columns"""
-    return M[LOGICAL, :]
-
-def _block_64x9(M):
-    """all rows, cols = LOGICAL"""
-    return M[:, LOGICAL]
-
-
-def _fidelity(A: jnp.ndarray, B: jnp.ndarray) -> float:
-
-    """
-    Computes the fidelity between two square matrices A and B.
-    Fidelity is defined as:
-    F(A, B) = (Tr(A†A) + |Tr(B†A)|²) / (n * (n + 1))
-
-    updated from using just falttened inner product divided by product of norms.
-
-    # inner = jnp.vdot(A, B)
-    # return jnp.abs(inner) / (jnp.linalg.norm(A) * jnp.linalg.norm(B))
-
-    """
-
-    if A.shape != B.shape or A.ndim != 2 or A.shape[0] != A.shape[1]:
-        raise ValueError("A and B must be square matrices of the same size")
-
-    n = A.shape[0]
-
-    # --- core computation ----------------------------------------------------
-    term1 = jnp.trace(A.conj().T @ A)                 # Tr(A†A)
-    term2 = jnp.trace(B.conj().T @ A)                 # Tr(B†A)
-    fidelity_val = (term1 + abs(term2)**2) / (n * (n + 1))
-
-    # If you prefer a plain Python float:
-    return float(np.real_if_close(fidelity_val))
-# ----------------------------------------------------------------------
-def frobenius_diff(A: np.ndarray, B: np.ndarray) -> float:
-    diff = A - B
-    return np.linalg.norm(A-B, ord='fro')
-# ----------------------------------------------------------------------
-
-
-# jit-compiled apply
-@jit
-def _apply_jax(U: jnp.ndarray, pair: int, p: float) -> jnp.ndarray:
-    H = H_BASE[pair]
-    U_gate = jax.scipy.linalg.expm(-1j * jnp.pi * p * H)
-    return U_gate @ U
-
-@partial(jit, static_argnums=(2,))
-def _vec_jax(U: jnp.ndarray, step: int, mode: str) -> jnp.ndarray:
-    parts = []
-    if mode in ("full", "both"):
-        parts += [jnp.real(U).ravel(), jnp.imag(U).ravel()]
-    if mode in ("block", "both"):
-        blk = U[LOGICAL][:, LOGICAL]
-        parts += [jnp.real(blk).ravel(), jnp.imag(blk).ravel()]
-    parts += [jnp.array([step], dtype=jnp.float32)]
-    return jnp.concatenate(parts).astype(jnp.float32)
-_apply_batched = jit(vmap(_apply_jax, in_axes=(0, None, None), out_axes=0))
-_vec_batched   = jit(vmap(_vec_jax,   in_axes=(0, None, None), out_axes=0), static_argnums=(2,))
-
-
-def reward_fn(U_now, U_prev, step, max_depth):
-    """
-        Implements the 10 bullet-points exactly (vectorised & JIT-safe).
-        Returns  (reward, done)
-        1. For each step if the fidelity of the 64x64 is increased , +1 to the reward.
-        2. For each step if the fidelity of the 64x64 is increased by .1, +5 reward.
-        3. For each step if the fidelity of the 64x64 is increased by .2, +7 reward.
-
-        9x9 indices are [9,10,12,17,18,20,33,34,36](zero based).
-        4. For each step if the fidelity of the 9x9 is increased, +.5 to the reward.
-        5. For each step if the fidelity of the 9x9 is increased by .1, +2 reward.
-        6. For each step if the fidelity of the 9x9 is increased by .2, +3 reward.
-
-        9x64 block is [(9,x),(10,x),(12,x),(17,x),(18,x),(20,x),(33,x),(34,x),(36,x)] for x in range(64).
-        7. For each step if the forbenius distance between the 9x64 block and the target matrix's 9x64 block is decresed compared to the previous step, then +2 reward.
-        8. For each step if the forbenius distance between the 9x64 block and the target matrix's 9x64 block is decresed by >.2 compared to the previous step, then +4 reward.
-        9. For each step if the forbenius distance between the 9x64 block and the target matrix's 9x64 block is decresed by 0 compared to the previous step, then +5 reward.
-        64x9 block is [(x,9),(x,10),(x,12),(x,17),(x,18),(x,20),(x,33),(x,34),(x,36)] for x in range(64).
-        10. For each step if the forbenius distance between the 64x9 block and the target matrix's 64x9 block is decresed compared to the previous step, then +2 reward.
-        11. For each step if the forbenius distance between the 64x9 block and the target matrix's 64x9 block is decresed by >.2 compared to the previous step, then +4 reward.
-        12. For each step if the forbenius distance between the 64x9 block and the target matrix's 64x9 block is decresed by 0 compared to the previous step, then +5 reward.
-
-        13. For each step if the forbenius distance between both the 9x64 block and 64x9 block and the target matrix's 9x64 block and the 64x9 block is 0, then +100 reward and exit.
-        
-        14. For each step if the forbenius distance between the 9x64 block and the target matrix's 9x64 block is increased compared to the previous step, then -1 reward.
-        15. For each step if the forbenius distance between the 64x9 block and the target matrix's 9x64 block is increased compared to the previous step, then -1 reward.
-        16. For each step if the fidelity with the target matrix is decresed compared to the previous step, then -1 reward.
-        17. For each step if the fidelity with the target matrix is decresed by .1 compared to the previous step, then -2 reward.
-        18. For each step if the fidelity with the target matrix is decresed by .2 compared to the previous step, then -3 reward.
-        19. For each step after the 8th step, step penalty of -(sqrt(step_count)).
-        20. If the fidelity is greater than .96 or forb norm rectangular blocks < .03, +100 reward and exit.
-    """
-    # ---------- fidelity 64 × 64 & 9 × 9 ----------------------------------
-    f64_now  = _fidelity(U_now,  TARGET_FULL)
-    f64_prev = _fidelity(U_prev, TARGET_FULL)
-
-    f9_now   = _fidelity(_block(U_now),  TARGET_BLOCK)
-    f9_prev  = _fidelity(_block(U_prev), TARGET_BLOCK)
-
-    df64 = f64_now - f64_prev
-    df9  = f9_now  - f9_prev
-
-    # ---------- Frobenius distances for 9×64 and 64×9 ---------------------
-    frob9x64_now  = float(jnp.linalg.norm(_block_9x64(U_now)  - _block_9x64(TARGET_FULL)))
-    frob9x64_prev = float(jnp.linalg.norm(_block_9x64(U_prev) - _block_9x64(TARGET_FULL)))
-    d_frob9x64    = frob9x64_prev - frob9x64_now          # >0 means “closer”
-
-    frob64x9_now  = float(jnp.linalg.norm(_block_64x9(U_now)  - _block_64x9(TARGET_FULL)))
-    frob64x9_prev = float(jnp.linalg.norm(_block_64x9(U_prev) - _block_64x9(TARGET_FULL)))
-    d_frob64x9    = frob64x9_prev - frob64x9_now
-
-    # ---------- build reward ---------------------------------------------
-    r = 0.0
-    # 1-3  (64×64 fidelity improvements)
-    if df64  > 0:    r += 1.0
-    if df64 >= .1:   r += 2.0
-    if df64 >= .2:   r += 3.0
-    # 4-6  (9×9 block fidelity improvements)
-    if df9   > 0:    r += 0.5
-    if df9  >= .1:   r += 1.0
-    if df9  >= .2:   r += 2.0
-    # 7-9  (9×64 Frobenius ↓)
-    if d_frob9x64  > 0  :   r += 1.0
-    if d_frob9x64  > .09:   r += 2.0
-    if d_frob9x64  > .2 :   r += 4.0
-    if frob9x64_now == 0:   r += 8.0
-    # 10-12 (64×9 Frobenius ↓)
-    if d_frob64x9  > 0  :   r += 1.0
-    if d_frob64x9  > .09:   r += 2.0
-    if d_frob64x9  > .2 :   r += 4.0
-    if frob64x9_now == 0:   r += 8.0
-    # 13 perfect match of both cross-blocks
-    # if (frob9x64_now == 0) and (frob64x9_now == 0):
-    if (frob9x64_now == 0) and (frob64x9_now == 0):
-        r   += 100.0
-        done = True
-        return r, done, f64_now, f9_now, frob9x64_now, frob64x9_now
-    # 14-15 penalties (Frobenius ↑)
-    if d_frob9x64  < 0: r -= 1.0
-    if d_frob64x9  < 0: r -= 1.0
-    # 16-18 penalties (64×64 fidelity ↓)
-    if df64  < 0:      r -= 1.0
-    if df64 <= -.1:    r -= 2.0
-    if df64 <= -.2:    r -= 3.0
-    # 19 step-penalty after 8th step
-    if step > 8:
-        r -= math.sqrt(step)
-    # 20 near-perfect overall fidelity
-    # if f64_now >= .96 or (d_frob64x9 <= 0.03 and d_frob9x64 <= 0.03):
-    if f64_now >= .96:
-        r += 100.0
-        done = True
-    else:
-        done = (step >= max_depth)
-
-    # r+=1
-
-    return r, done, f64_now, f9_now
-
-# ----------------------------------------------------------------------
-# 2. Gym-compatible environment for DI-engine PDQN
-# ----------------------------------------------------------------------
-MASK_LEN = len(NEIGHBORS)   
-@ENV_REGISTRY.register('ExchangeCNOTEnvDI')
-class ExchangeCNOTEnvDI(gym.Env):
-    """
-    Parameter-ised exchange-gate environment **forbidding consecutive
-    repeats of the same qubit pair** via an action-mask.
-
-    – The mask lives in both the observation tail **and** in `info["action_mask"]`.
-    – If the agent violates the mask we return a penalty (or raise).
-    – Added: neat episode-summary prints that show the gate sequence,
-             cumulative reward and fidelity after every episode.
-    """
-
-    metadata = {"render.modes": ["human"]}
-
-    # ------------------------------------------------------------------
-    # ctor
-    # ------------------------------------------------------------------
-    def __init__(self, max_depth: int = 18, obs_mode: str = "block",
-                 cfg: dict | None = None, **_unused):
-        # DI-engine passes the whole cfg dict – pick out what we need
-        if cfg is not None:
-            max_depth = cfg.get("max_depth", max_depth)
-            obs_mode  = cfg.get("obs_mode",  obs_mode)
-
-        super().__init__()
-        assert obs_mode in ("block", "full", "both")
-
-        # ---------- episode-print helpers ----------
-        self._ep_idx     = 0
-        self._seq        = []
-        # self._cum_reward = 0.0
-        self._last_fid   = float("nan")   # filled at episode end
-        # -------------------------------------------
-
-        self.max_depth  = int(max_depth)
-        self.obs_mode   = obs_mode
-
-        # action = (discrete pair, continuous p ∈ [-2,2])
-        self.action_space = spaces.Tuple((
-            spaces.Discrete(MASK_LEN),
-            spaces.Box(low=-2.0, high=2.0, shape=(1,), dtype=jnp.float32)
-        ))
-        self.reward_space = spaces.Box(low=-jnp.inf, high=jnp.inf, shape=(), dtype=jnp.float32)
-
-        # observation dim
-        dim = 1                                      # step counter
-        if obs_mode in ("full", "both"):   dim += 64 * 64 * 2
-        if obs_mode in ("block", "both"):  dim += 9 * 9 * 2
-        dim += MASK_LEN                               # 5-bit mask
-        self.observation_space = spaces.Box(low=-jnp.inf, high=jnp.inf,
-                                            shape=(dim,), dtype=jnp.float32)
-
-        # env state
-        self.U              = None
-        self.step_count     = None
-        self.last_pair      = None
-        self.valid_mask     = None
-        self._episode_return = 0.0
-
-    # ------------------------------------------------------------------
-    # helpers
-    # ------------------------------------------------------------------
-    def _make_obs(self) -> jnp.ndarray:
-        base = _vec_jax(jnp.array(self.U), self.step_count, self.obs_mode)
-        return _to_np(jnp.concatenate([base, self.valid_mask]))
-
-    # ------------------------------------------------------------------
-    # Gym API
-    # ------------------------------------------------------------------
-    def reset(self, *_, **__) -> jnp.ndarray:
-        """Start a fresh episode – and *after* the previous one, print summary."""
-        # ------- print the previous episode (if any) -------
-        if self._seq:           # only if we actually finished one episode
-            print(f"Episode {self._ep_idx:03d} │ "
-                  f"reward = {self._episode_return:+.3f} │ "
-                  f"fidelity = {self._last_fid:.6f}\n"
-                  f"Sequence: {self._seq}\n")
-        # ------- clear trackers for the new episode -------
-        self._ep_idx     += 1
-        self._seq         = []
-        # self._cum_reward  = 0.0
-        self._last_fid    = float("nan")
-        # ---------------------------------------------------
-
-        self.U = jnp.eye(2 ** N_PHYS, dtype=jnp.complex64)
-        phase  = jnp.vdot(self.U, TARGET_FULL)
-        self.U *= jnp.conj(phase / jnp.abs(phase))
-
-        self.step_count      = 0
-        self.last_pair       = None
-        self.valid_mask      = jnp.ones(MASK_LEN, dtype=jnp.float32)
-        self._episode_return = 0.0
-        return self._make_obs()
-
-    def step(self, action):
-        # --------------------------------------------------------------
-        # 1. canonicalise incoming action
-        # --------------------------------------------------------------
-        if isinstance(action, dict):                # DI-engine style
-            pair_idx = int(action.get("action_type",
-                          action.get("action", action.get("action_index"))))
-            p_val    = action.get("action_args", action.get("param", [0.0]))
-            p        = float(p_val[0] if isinstance(p_val, (list, tuple, jnp.ndarray)) else p_val)
-        else:                                       # tuple / list / ndarray
-            pair_idx, p = int(action[0]), float(action[1])
-
-        # --------------------------------------------------------------
-        # 2. legal-action check (mask violation ⇒ penalty timestep)
-        # --------------------------------------------------------------
-        if self.valid_mask[pair_idx] == 0:
-            self._episode_return += INVALID_PENALTY
-            obs  = self._make_obs()               # mask unchanged
-            info = dict(
-                invalid_action=True,
-                action_mask=_to_np(self.valid_mask),
-                eval_episode_return=float(self._episode_return)
-            )
-            return Timestep(obs, float(INVALID_PENALTY), False, info)
-
-        # --------------------------------------------------------------
-        # 3. normal transition
-        # --------------------------------------------------------------
-        U_prev      = self.U
-        U_batch     = self.U[jnp.newaxis, ...]             # (1,64,64)
-        self.U      = _apply_batched(U_batch, pair_idx, p)[0]
-
-        self.step_count += 1
-        self.last_pair   = pair_idx
-        self.valid_mask  = jnp.ones(MASK_LEN, dtype=jnp.float32).at[pair_idx].set(0)
-
-        r, done, fid64, fid9 = reward_fn(self.U, U_prev, self.step_count, self.max_depth)
-        self._episode_return += r
-
-        # track episode details for printing
-        self._seq.append((pair_idx, p))
-        if done:
-            self._last_fid = float(fid9)          # choose whichever fidelity you care about
-
-        obs  = self._make_obs()
-        info = dict(
-            fid64=float(fid64),
-            fid9=float(fid9),
-            action_mask=_to_np(self.valid_mask),
-            eval_episode_return=float(self._episode_return)
-        )
-        print(f"[debug] step {self.step_count}  raw_reward={r}")
-        return Timestep(obs, float(r), done, info)
-
-    # ------------------------------------------------------------------
-    # misc glue
-    # ------------------------------------------------------------------
-    def seed(self, seed: int | None = None, dynamic_seed: bool = False):
-        if seed is not None:
-            self._rng = jax.random.PRNGKey(int(seed))
-        return [seed]
-
-    @classmethod
-    def create_collector_env_cfg(cls, cfg: EasyDict):
-        base = dict(max_depth=cfg.max_episode_steps,
-                    obs_mode=cfg.get("obs_mode", "block"))
-        return [deepcopy(base) for _ in range(cfg.collector_env_num)]
-
-    @classmethod
-    def create_evaluator_env_cfg(cls, cfg: EasyDict):
-        base = dict(max_depth=cfg.max_episode_steps,
-                    obs_mode=cfg.get("obs_mode", "block"))
-        return [deepcopy(base) for _ in range(cfg.evaluator_env_num)]
-
-    def render(self, mode="human"):
-        print(f"Step {self.step_count:2d} | fidelity64 = {_fidelity(self.U, TARGET_FULL):.4f}")
-
-    def close(self):
-        pass
diff --git a/QCRL_v0.3/fw_target.py b/QCRL_v0.3/fw_target.py
deleted file mode 100644
index b14e7a8..0000000
--- a/QCRL_v0.3/fw_target.py
+++ /dev/null
@@ -1,38 +0,0 @@
-import math
-import jax.numpy as jnp
-import numpy as np
-from ex_operations import exchange_gate_nqubits
-
-p1 = math.acos(-1/math.sqrt(3)) / math.pi
-p2 = math.asin(1/3) / math.pi
-gate_specs = [
-    ( 1+p1,  [3,4] ),
-    # ( p1,    [3,4] ),
-    ( p2,    [4,5] ),
-    ( 0.5,   [2,3] ),
-    ( 1.0,   [3,4] ),
-    (-0.5,   [2,3] ),
-    (-0.5,   [4,5] ),
-    ( 1.0,   [1,2] ),
-    (-0.5,   [3,4] ),
-    (-0.5,   [2,3] ),
-    ( 1.0,   [4,5] ),
-    (-0.5,   [1,2] ),
-    ( 0.5,   [3,4] ),
-    (-0.5,   [2,3] ),
-    ( 1.0,   [4,5] ),
-    ( 1.0,   [1,2] ),
-    (-0.5,   [3,4] ),
-    (-0.5,   [2,3] ),
-    (-0.5,   [4,5] ),
-    ( 1.0,   [3,4] ),
-    ( 0.5,   [2,3] ),
-    ( 1-p2,  [4,5] ),
-    # ( -p1,   [3,4] ),
-    ( 1-p1,  [3,4] ),
-]
-
-U_circuit = jnp.eye(64, dtype=complex)
-for (p, (i, j)) in gate_specs:
-    U_gate = exchange_gate_nqubits(6, p, i, j)
-    U_circuit = U_gate @ U_circuit
\ No newline at end of file
diff --git a/QCRL_v0.3/inverse_rl/__init__.py b/QCRL_v0.3/inverse_rl/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/QCRL_v0.3/inverse_rl/airl.py b/QCRL_v0.3/inverse_rl/airl.py
deleted file mode 100644
index fa20194..0000000
--- a/QCRL_v0.3/inverse_rl/airl.py
+++ /dev/null
@@ -1,93 +0,0 @@
-from discriminators.base import Discriminator
-from networks.discriminator_network import G, H
-import torch
-import torch.nn as nn
-
-
-class AIRL(Discriminator):
-    def __init__(self, writer, device, state_dim, action_dim, args):
-        super(AIRL, self).__init__()
-        self.writer = writer
-        self.device = device
-        self.args = args
-
-        self.g = G(
-            self.args.state_only,
-            self.args.layer_num,
-            state_dim,
-            action_dim,
-            self.args.hidden_dim,
-            self.args.activation_function,
-            self.args.last_activation,
-        )
-        self.h = H(
-            self.args.layer_num,
-            state_dim,
-            action_dim,
-            self.args.hidden_dim,
-            self.args.activation_function,
-            self.args.last_activation,
-        )
-        self.criterion = nn.BCELoss()
-        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.args.lr)
-
-    def get_f(self, state, action, next_state, done_mask):
-        return self.g(state, action) + done_mask.float() * (
-            self.args.gamma * self.h(next_state) - self.h(state)
-        )
-
-    def get_d(self, log_prob, state, action, next_state, done_mask):
-        exp_f = torch.exp(self.get_f(state, action, next_state, done_mask))
-        return exp_f / (exp_f + torch.exp(log_prob))
-
-    def get_reward(self, log_prob, state, action, next_state, done):
-        done_mask = 1 - done.float()
-        # return (self.get_f(state,action,next_state,done_mask) - log_prob).detach()
-        d = (self.get_d(log_prob, state, action, next_state, done_mask)).detach()
-        return torch.log(d + 1e-3) - torch.log((1 - d) + 1e-3)
-
-    def forward(self, log_prob, state, action, next_state, done_mask):
-        d = self.get_d(log_prob, state, action, next_state, done_mask)
-        return d
-
-    def train_network(
-        self,
-        writer,
-        n_epi,
-        agent_s,
-        agent_a,
-        agent_next_s,
-        agent_log_prob,
-        agent_done_mask,
-        expert_s,
-        expert_a,
-        expert_next_s,
-        expert_log_prob,
-        expert_done_mask,
-    ):
-
-        expert_preds = self.forward(
-            expert_log_prob, expert_s, expert_a, expert_next_s, expert_done_mask
-        )
-        expert_loss = self.criterion(
-            expert_preds, torch.ones(expert_preds.shape[0], 1).to(self.device)
-        )
-
-        agent_preds = self.forward(
-            agent_log_prob, agent_s, agent_a, agent_next_s, agent_done_mask
-        )
-        agent_loss = self.criterion(
-            agent_preds, torch.zeros(agent_preds.shape[0], 1).to(self.device)
-        )
-
-        loss = expert_loss + agent_loss
-        expert_acc = ((expert_preds > 0.5).float()).mean()
-        learner_acc = ((agent_preds < 0.5).float()).mean()
-
-        if self.writer != None:
-            self.writer.add_scalar("loss/discriminator_loss", loss.item(), n_epi)
-        # if (expert_acc > 0.8) and (learner_acc > 0.8):
-        #    return
-        self.optimizer.zero_grad()
-        loss.backward()
-        self.optimizer.step()
\ No newline at end of file
diff --git a/QCRL_v0.3/inverse_rl/discriminator_network.py b/QCRL_v0.3/inverse_rl/discriminator_network.py
deleted file mode 100644
index ae250c2..0000000
--- a/QCRL_v0.3/inverse_rl/discriminator_network.py
+++ /dev/null
@@ -1,216 +0,0 @@
-from networks.base import Network
-
-import torch
-import torch.nn as nn
-
-
-class VDB(nn.Module):
-    def __init__(self, state_dim, action_dim, hidden_dim, z_dim):
-        super(VDB, self).__init__()
-        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)
-        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
-        self.mu = nn.Linear(hidden_dim, z_dim)
-        self.sigma = nn.Linear(hidden_dim, z_dim)
-
-    def get_z(self, x):
-        x = torch.relu(self.fc1(x))
-        x = torch.relu(self.fc2(x))
-        mu = self.mu(x)
-        sigma = self.sigma(x)
-        std = torch.exp(sigma / 2)
-        eps = torch.randn_like(std)
-        return mu + std * eps, mu, sigma
-
-    def get_mean(self, x):
-        x = torch.relu(self.fc1(x))
-        x = torch.relu(self.fc2(x))
-        mu = self.mu(x)
-        return mu
-
-
-class G(Network):
-    def __init__(
-        self,
-        state_only,
-        layer_num,
-        input_dim,
-        output_dim,
-        hidden_dim,
-        activation_function,
-        last_activation=None,
-    ):
-        if state_only:
-            super(G, self).__init__(
-                layer_num,
-                input_dim,
-                1,
-                hidden_dim,
-                activation_function,
-                last_activation,
-            )
-        else:
-            super(G, self).__init__(
-                layer_num,
-                input_dim + output_dim,
-                1,
-                hidden_dim,
-                activation_function,
-                last_activation,
-            )
-        self.state_only = state_only
-
-    def forward(self, state, action):
-        if self.state_only:
-            x = state
-        else:
-            x = torch.cat((state, action), -1)
-        return self._forward(x)
-
-
-class H(Network):
-    def __init__(
-        self,
-        layer_num,
-        input_dim,
-        output_dim,
-        hidden_dim,
-        activation_function,
-        last_activation=None,
-    ):
-        super(H, self).__init__(
-            layer_num, input_dim, 1, hidden_dim, activation_function, last_activation
-        )
-
-    def forward(self, x):
-        return self._forward(x)
-
-
-class VariationalG(nn.Module):
-    def __init__(self, state_dim, action_dim, hidden_dim, z_dim, state_only=True):
-        super(VariationalG, self).__init__()
-        if state_only:
-            self.vdb = VDB(state_dim, 0, hidden_dim, z_dim)
-        else:
-            self.vdb = VDB(state_dim, action_dim, hidden_dim, z_dim)
-        self.state_only = state_only
-
-        self.fc3 = nn.Linear(z_dim, 1)
-
-    def forward(self, state, action, get_dist=False):
-        if self.state_only:
-            x = state
-        else:
-            x = torch.cat((state, action), -1)
-        z, mu, std = self.vdb.get_z(x)
-        x = torch.sigmoid(self.fc3(z))
-        if get_dist == False:
-            return x
-        else:
-            return x, mu, std
-
-
-class VariationalH(nn.Module):
-    def __init__(self, state_dim, action_dim, hidden_dim, z_dim):
-        super(VariationalH, self).__init__()
-        self.vdb = VDB(state_dim, 0, hidden_dim, z_dim)
-        self.fc3 = nn.Linear(z_dim, 1)
-
-    def forward(self, state, get_dist=False):
-        z, mu, std = self.vdb.get_z(state)
-        x = torch.sigmoid(self.fc3(z))
-        if get_dist == False:
-            return x
-        else:
-            return x, mu, std
-
-
-class Q_phi(Network):
-    def __init__(
-        self,
-        layer_num,
-        input_dim,
-        output_dim,
-        hidden_dim,
-        activation_function,
-        last_activation=None,
-        trainable_std=False,
-    ):
-        """
-        self.q_network
-        input : s, next_s
-        output : mean,std
-        target : action
-        """
-        self.trainable_std = trainable_std
-        if self.trainable_std == True:
-            self.logstd = nn.Parameter(torch.zeros(1, output_dim))
-        super(Q_phi, self).__init__(
-            layer_num,
-            input_dim * 2,
-            output_dim,
-            hidden_dim,
-            activation_function,
-            last_activation,
-        )
-
-    def forward(self, state, next_state):
-        x = torch.cat((state, next_state), -1)
-        mu = self._forward(x)
-        if self.trainable_std == True:
-            std = torch.exp(self.logstd)
-        else:
-            logstd = torch.zeros_like(mu)
-            std = torch.exp(logstd)
-        return mu, std
-
-
-class Empowerment(Network):
-    def __init__(
-        self,
-        layer_num,
-        input_dim,
-        output_dim,
-        hidden_dim,
-        activation_function,
-        last_activation=None,
-    ):
-        super(Empowerment, self).__init__(
-            layer_num, input_dim, 1, hidden_dim, activation_function, last_activation
-        )
-        """
-        self.phi
-        input : s
-        output : scalar
-        """
-
-    def forward(self, x):
-        return self._forward(x)
-
-
-class Reward(Network):
-    def __init__(
-        self,
-        layer_num,
-        input_dim,
-        output_dim,
-        hidden_dim,
-        activation_function,
-        last_activation=None,
-    ):
-        super(Reward, self).__init__(
-            layer_num,
-            input_dim + output_dim,
-            1,
-            hidden_dim,
-            activation_function,
-            last_activation,
-        )
-        """
-        self.reward
-        input : s,a
-        output : scalar
-        """
-
-    def forward(self, state, action):
-        x = torch.cat((state, action), -1)
-        return self._forward(x)
\ No newline at end of file
diff --git a/QCRL_v0.3/inverse_rl/network_base.py b/QCRL_v0.3/inverse_rl/network_base.py
deleted file mode 100644
index 629c756..0000000
--- a/QCRL_v0.3/inverse_rl/network_base.py
+++ /dev/null
@@ -1,54 +0,0 @@
-from abc import *
-
-import torch
-import torch.nn as nn
-
-
-class NetworkBase(nn.Module, metaclass=ABCMeta):
-    @abstractmethod
-    def __init__(self):
-        super(NetworkBase, self).__init__()
-
-    @abstractmethod
-    def forward(self, x):
-        return x
-
-
-class Network(NetworkBase):
-    def __init__(
-        self,
-        layer_num,
-        input_dim,
-        output_dim,
-        hidden_dim,
-        activation_function=torch.relu,
-        last_activation=None,
-    ):
-        super(Network, self).__init__()
-        self.activation = activation_function
-        self.last_activation = last_activation
-        layers_unit = [input_dim] + [hidden_dim] * (layer_num - 1)
-        layers = [
-            nn.Linear(layers_unit[idx], layers_unit[idx + 1])
-            for idx in range(len(layers_unit) - 1)
-        ]
-        self.layers = nn.ModuleList(layers)
-        self.last_layer = nn.Linear(layers_unit[-1], output_dim)
-        self.network_init()
-
-    def forward(self, x):
-        return self._forward(x)
-
-    def _forward(self, x):
-        for layer in self.layers:
-            x = self.activation(layer(x))
-        x = self.last_layer(x)
-        if self.last_activation != None:
-            x = self.last_activation(x)
-        return x
-
-    def network_init(self):
-        for layer in self.modules():
-            if isinstance(layer, nn.Linear):
-                nn.init.orthogonal_(layer.weight)
-                layer.bias.data.zero_()
\ No newline at end of file
diff --git a/QCRL_v0.3/main.ipynb b/QCRL_v0.3/main.ipynb
deleted file mode 100644
index 31a3d49..0000000
--- a/QCRL_v0.3/main.ipynb
+++ /dev/null
@@ -1,93 +0,0 @@
-{
- "cells": [
-  {
-   "cell_type": "code",
-   "execution_count": 1,
-   "id": "ea68f2ca",
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "/home/schiffer98/QCRL/.venv/lib/python3.10/site-packages/treevalue/tree/integration/torch.py:23: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
-      "  register_for_torch(TreeValue)\n",
-      "/home/schiffer98/QCRL/.venv/lib/python3.10/site-packages/treevalue/tree/integration/torch.py:24: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
-      "  register_for_torch(FastTreeValue)\n",
-      "/home/schiffer98/QCRL/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
-      "  from .autonotebook import tqdm as notebook_tqdm\n",
-      "/home/schiffer98/QCRL/.venv/lib/python3.10/site-packages/ding/torch_utils/data_helper.py:194: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
-      "  return torch.from_numpy(item).to(dtype)\n",
-      "/home/schiffer98/QCRL/QCRL_v0.2/exch_gym_env.py:308: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
-      "  pair_idx = int(action.get(\"action_type\",\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "[debug] step 1  raw_reward=-0.5\n"
-     ]
-    },
-    {
-     "ename": "KeyboardInterrupt",
-     "evalue": "",
-     "output_type": "error",
-     "traceback": [
-      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
-      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
-      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mding\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mentry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m serial_pipeline\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m main_config, create_config\n\u001b[0;32m---> 16\u001b[0m \u001b[43mserial_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmain_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_config\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmax_env_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m     \u001b[38;5;66;03m# whatever quick smoke-test length you need\u001b[39;00m\n",
-      "File \u001b[0;32m~/QCRL/.venv/lib/python3.10/site-packages/ding/entry/serial_entry.py:107\u001b[0m, in \u001b[0;36mserial_pipeline\u001b[0;34m(input_cfg, seed, env_setting, model, max_train_iter, max_env_step, dynamic_seed)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Evaluate policy performance\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluator\u001b[38;5;241m.\u001b[39mshould_eval(learner\u001b[38;5;241m.\u001b[39mtrain_iter):\n\u001b[0;32m--> 107\u001b[0m     stop, eval_info \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop:\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
-      "File \u001b[0;32m~/QCRL/.venv/lib/python3.10/site-packages/ding/worker/collector/interaction_serial_evaluator.py:238\u001b[0m, in \u001b[0;36mInteractionSerialEvaluator.eval\u001b[0;34m(self, save_ckpt_fn, train_iter, envstep, n_episode, force_render, policy_kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_states \u001b[38;5;241m=\u001b[39m [output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m policy_output\u001b[38;5;241m.\u001b[39mvalues()]\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m     policy_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m actions \u001b[38;5;241m=\u001b[39m {i: a[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i, a \u001b[38;5;129;01min\u001b[39;00m policy_output\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    240\u001b[0m actions \u001b[38;5;241m=\u001b[39m to_ndarray(actions)\n",
-      "File \u001b[0;32m~/QCRL/.venv/lib/python3.10/site-packages/ding/policy/pdqn.py:570\u001b[0m, in \u001b[0;36mPDQNPolicy._forward_eval\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    568\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_model\u001b[38;5;241m.\u001b[39mforward(inputs, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompute_discrete\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cuda:\n\u001b[0;32m--> 570\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mto_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m output \u001b[38;5;241m=\u001b[39m default_decollate(output)\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {i: d \u001b[38;5;28;01mfor\u001b[39;00m i, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(data_id, output)}\n",
-      "File \u001b[0;32m~/QCRL/.venv/lib/python3.10/site-packages/ding/torch_utils/data_helper.py:68\u001b[0m, in \u001b[0;36mto_device\u001b[0;34m(item, device, ignore_keys)\u001b[0m\n\u001b[1;32m     66\u001b[0m             new_item[k] \u001b[38;5;241m=\u001b[39m item[k]\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m             new_item[k] \u001b[38;5;241m=\u001b[39m \u001b[43mto_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_item\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, numbers\u001b[38;5;241m.\u001b[39mIntegral) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, numbers\u001b[38;5;241m.\u001b[39mReal):\n",
-      "File \u001b[0;32m~/QCRL/.venv/lib/python3.10/site-packages/ding/torch_utils/data_helper.py:68\u001b[0m, in \u001b[0;36mto_device\u001b[0;34m(item, device, ignore_keys)\u001b[0m\n\u001b[1;32m     66\u001b[0m             new_item[k] \u001b[38;5;241m=\u001b[39m item[k]\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m             new_item[k] \u001b[38;5;241m=\u001b[39m \u001b[43mto_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_item\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, numbers\u001b[38;5;241m.\u001b[39mIntegral) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, numbers\u001b[38;5;241m.\u001b[39mReal):\n",
-      "File \u001b[0;32m~/QCRL/.venv/lib/python3.10/site-packages/ding/torch_utils/data_helper.py:56\u001b[0m, in \u001b[0;36mto_device\u001b[0;34m(item, device, ignore_keys)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m item\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mitem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, Sequence):\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n",
-      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
-     ]
-    }
-   ],
-   "source": [
-    "import logging, os, multiprocessing as mp\n",
-    "\n",
-    "# 1) make every logger inherit this blanket rule\n",
-    "logging.disable(logging.CRITICAL)        # blocks everything ≤ CRITICAL\n",
-    "\n",
-    "# 2) be sure child processes inherit the rule (spawn, not fork)\n",
-    "mp.set_start_method(\"spawn\", force=True)\n",
-    "\n",
-    "# 3) optional: stop the colour codes DI-engine adds\n",
-    "os.environ[\"DI_DISABLE_COLOR\"] = \"true\"\n",
-    "\n",
-    "# now import / run DI-engine\n",
-    "from ding.entry import serial_pipeline\n",
-    "from config import main_config, create_config\n",
-    "\n",
-    "serial_pipeline([main_config, create_config],\n",
-    "                seed=42,\n",
-    "                max_env_step=100)     # whatever quick smoke-test length you need\n"
-   ]
-  }
- ],
- "metadata": {
-  "kernelspec": {
-   "display_name": ".venv",
-   "language": "python",
-   "name": "python3"
-  },
-  "language_info": {
-   "codemirror_mode": {
-    "name": "ipython",
-    "version": 3
-   },
-   "file_extension": ".py",
-   "mimetype": "text/x-python",
-   "name": "python",
-   "nbconvert_exporter": "python",
-   "pygments_lexer": "ipython3",
-   "version": "3.10.12"
-  }
- },
- "nbformat": 4,
- "nbformat_minor": 5
-}
diff --git a/policy.pdqn.py b/policy.pdqn.py
deleted file mode 100644
index 6b66e26..0000000
--- a/policy.pdqn.py
+++ /dev/null
@@ -1,527 +0,0 @@
-from typing import List, Dict, Any, Tuple
-from collections import namedtuple
-import copy
-import torch
-
-from ding.torch_utils import Adam, to_device
-from ding.rl_utils import q_nstep_td_data, q_nstep_td_error, get_nstep_return_data, get_train_sample
-from ding.model import model_wrap
-from ding.utils import POLICY_REGISTRY
-from ding.utils.data import default_collate, default_decollate
-from .base_policy import Policy
-from .common_utils import default_preprocess_learn
-
-
-@POLICY_REGISTRY.register('pdqn')
-class PDQNPolicy(Policy):
-    """
-    Overview:
-        Policy class of PDQN algorithm, which extends the DQN algorithm on discrete-continuous hybrid action spaces.
-        Paper link: https://arxiv.org/abs/1810.06394.
-
-    Config:
-        == ==================== ======== ============== ======================================== =======================
-        ID Symbol               Type     Default Value  Description                              Other(Shape)
-        == ==================== ======== ============== ======================================== =======================
-        1  ``type``             str      pdqn           | RL policy register name, refer to      | This arg is optional,
-                                                        | registry ``POLICY_REGISTRY``           | a placeholder
-        2  ``cuda``             bool     False          | Whether to use cuda for network        | This arg can be diff-
-                                                                                                 | erent from modes
-        3  ``on_policy``        bool     False          | Whether the RL algorithm is on-policy  | This value is always
-                                                        | or off-policy                          | False for PDQN
-        4  ``priority``         bool     False          | Whether use priority(PER)              | Priority sample,
-                                                                                                 | update priority
-        5  | ``priority_IS``    bool     False          | Whether use Importance Sampling Weight
-           | ``_weight``                                | to correct biased update. If True,
-                                                        | priority must be True.
-        6  | ``discount_``      float    0.97,          | Reward's future discount factor, aka.  | May be 1 when sparse
-           | ``factor``                  [0.95, 0.999]  | gamma                                  | reward env
-
-        7  ``nstep``            int      1,             | N-step reward discount sum for target
-                                         [3, 5]         | q_value estimation
-        8  | ``learn.update``   int      3              | How many updates(iterations) to train  | This args can be vary
-           | ``per_collect``                            | after collector's one collection. Only | from envs. Bigger val
-                                                        | valid in serial training               | means more off-policy
-        9  | ``learn.batch_``   int      64             | The number of samples of an iteration
-           | ``size``
-           | ``_gpu``
-        11 | ``learn.learning`` float    0.001          | Gradient step length of an iteration.
-           | ``_rate``
-        12 | ``learn.target_``  int      100            | Frequence of target network update.    | Hard(assign) update
-           | ``update_freq``
-        13 | ``learn.ignore_``  bool     False          | Whether ignore done for target value   | Enable it for some
-           | ``done``                                   | calculation.                           | fake termination env
-        14 ``collect.n_sample`` int      [8, 128]       | The number of training samples of a    | It varies from
-                                                        | call of collector.                     | different envs
-        15 | ``collect.unroll`` int      1              | unroll length of an iteration          | In RNN, unroll_len>1
-           | ``_len``
-        16 | ``collect.noise``  float    0.1            | add noise to continuous args
-           | ``_sigma``                                 | during collection
-        17 | ``other.eps.type`` str      exp            | exploration rate decay type            | Support ['exp',
-                                                                                                 | 'linear'].
-        18 | ``other.eps.``     float    0.95           | start value of exploration rate        | [0,1]
-           | ``start``
-        19 | ``other.eps.``     float    0.05           | end value of exploration rate          | [0,1]
-           | ``end``
-        20 | ``other.eps.``     int      10000          | decay length of exploration            | greater than 0. set
-           | ``decay``                                                                           | decay=10000 means
-                                                                                                 | the exploration rate
-                                                                                                 | decay from start
-                                                                                                 | value to end value
-                                                                                                 | during decay length.
-        == ==================== ======== ============== ======================================== =======================
-    """
-    config = dict(
-        # (str) RL policy register name (refer to function "POLICY_REGISTRY").
-        type='pdqn',
-        # (bool) Whether to use cuda in policy.
-        cuda=False,
-        # (bool) Whether learning policy is the same as collecting data policy(on-policy).
-        on_policy=False,
-        # (bool) Whether to enable priority experience sample.
-        priority=False,
-        # (bool) Whether to use Importance Sampling Weight to correct biased update. If True, priority must be True.
-        priority_IS_weight=False,
-        # (float) Discount factor(gamma) for returns.
-        discount_factor=0.97,
-        # (int) The number of step for calculating target q_value.
-        nstep=1,
-        # learn_mode config
-        learn=dict(
-            # (int) How many updates(iterations) to train after collector's one collection.
-            # Bigger "update_per_collect" means bigger off-policy.
-            # collect data -> update policy-> collect data -> ...
-            update_per_collect=3,
-            # (int) How many samples in a training batch.
-            batch_size=64,
-            # (float) The step size of gradient descent.
-            learning_rate=0.001,
-            # (int) Frequence of target network update.
-            target_theta=0.005,
-            # (bool) Whether ignore done(usually for max step termination env).
-            # Note: Gym wraps the MuJoCo envs by default with TimeLimit environment wrappers.
-            # These limit HalfCheetah, and several other MuJoCo envs, to max length of 1000.
-            # However, interaction with HalfCheetah always gets done with done is False,
-            # Since we inplace done==True with done==False to keep
-            # TD-error accurate computation(``gamma * (1 - done) * next_v + reward``),
-            # when the episode step is greater than max episode step.
-            ignore_done=False,
-        ),
-        # collect_mode config
-        collect=dict(
-            # (int) How many training samples collected in one collection procedure.
-            # Only one of [n_sample, n_episode] shoule be set.
-            # n_sample=8,
-            # (int) Split episodes or trajectories into pieces with length `unroll_len`.
-            unroll_len=1,
-            # (float) It is a must to add noise during collection. So here omits noise and only set ``noise_sigma``.
-            noise_sigma=0.1,
-        ),
-        eval=dict(),  # for compatibility
-        # other config
-        other=dict(
-            # Epsilon greedy with decay.
-            eps=dict(
-                # (str) Decay type. Support ['exp', 'linear'].
-                type='exp',
-                # (float) Epsilon start value.
-                start=0.95,
-                # (float) Epsilon end value.
-                end=0.1,
-                # (int) Decay length(env step)
-                decay=10000,
-            ),
-            replay_buffer=dict(
-                # (int) Maximum size of replay buffer. Usually, larger buffer size is better.
-                replay_buffer_size=10000,
-            ),
-        ),
-    )
-
-    def default_model(self) -> Tuple[str, List[str]]:
-        """
-        Overview:
-            Return this algorithm default neural network model setting for demonstration. ``__init__`` method will \
-            automatically call this method to get the default model setting and create model.
-        Returns:
-            - model_info (:obj:`Tuple[str, List[str]]`): The registered model name and model's import_names.
-
-        .. note::
-            The user can define and use customized network model but must obey the same inferface definition indicated \
-            by import_names path. For example about PDQN, its registered name is ``pdqn`` and the import_names is \
-            ``ding.model.template.pdqn``.
-        """
-        return 'pdqn', ['ding.model.template.pdqn']
-
-    def _init_learn(self) -> None:
-        """
-        Overview:
-            Initialize the learn mode of policy, including related attributes and modules. For PDQN, it mainly \
-            contains two optimizers, algorithm-specific arguments such as nstep and gamma, main and target model.
-            This method will be called in ``__init__`` method if ``learn`` field is in ``enable_field``.
-
-        .. note::
-            For the member variables that need to be saved and loaded, please refer to the ``_state_dict_learn`` \
-            and ``_load_state_dict_learn`` methods.
-
-        .. note::
-            For the member variables that need to be monitored, please refer to the ``_monitor_vars_learn`` method.
-
-        .. note::
-            If you want to set some spacial member variables in ``_init_learn`` method, you'd better name them \
-            with prefix ``_learn_`` to avoid conflict with other modes, such as ``self._learn_attr1``.
-        """
-        self._priority = self._cfg.priority
-        self._priority_IS_weight = self._cfg.priority_IS_weight
-        # Optimizer
-        self._dis_optimizer = Adam(
-            list(self._model.dis_head.parameters()) + list(self._model.cont_encoder.parameters()),
-            # this is very important to put cont_encoder.parameters in here.
-            lr=self._cfg.learn.learning_rate_dis
-        )
-        self._cont_optimizer = Adam(list(self._model.cont_head.parameters()), lr=self._cfg.learn.learning_rate_cont)
-
-        self._gamma = self._cfg.discount_factor
-        self._nstep = self._cfg.nstep
-
-        # use model_wrapper for specialized demands of different modes
-        self._target_model = copy.deepcopy(self._model)
-        self._target_model = model_wrap(
-            self._target_model,
-            wrapper_name='target',
-            update_type='momentum',
-            update_kwargs={'theta': self._cfg.learn.target_theta}
-        )
-        self._learn_model = model_wrap(self._model, wrapper_name='hybrid_argmax_sample')
-        self._learn_model.reset()
-        self._target_model.reset()
-        self.cont_train_cnt = 0
-        self.disc_train_cnt = 0
-        self.train_cnt = 0
-
-    def _forward_learn(self, data: Dict[str, Any]) -> Dict[str, Any]:
-        """
-        Overview:
-            Policy forward function of learn mode (training policy and updating parameters). Forward means \
-            that the policy inputs some training batch data from the replay buffer and then returns the output \
-            result, including various training information such as loss, q value, target_q_value, priority.
-        Arguments:
-            - data (:obj:`List[Dict[int, Any]]`): The input data used for policy forward, including a batch of \
-                training samples. For each element in list, the key of the dict is the name of data items and the \
-                value is the corresponding data. Usually, the value is torch.Tensor or np.ndarray or there dict/list \
-                combinations. In the ``_forward_learn`` method, data often need to first be stacked in the batch \
-                dimension by some utility functions such as ``default_preprocess_learn``. \
-                For PDQN, each element in list is a dict containing at least the following keys: ``obs``, ``action``, \
-                ``reward``, ``next_obs``, ``done``. Sometimes, it also contains other keys such as ``weight`` \
-                and ``value_gamma``.
-        Returns:
-            - info_dict (:obj:`Dict[str, Any]`): The information dict that indicated training result, which will be \
-                recorded in text log and tensorboard, values must be python scalar or a list of scalars. For the \
-                detailed definition of the dict, refer to the code of ``_monitor_vars_learn`` method.
-
-        .. note::
-            The input value can be torch.Tensor or dict/list combinations and current policy supports all of them. \
-            For the data type that not supported, the main reason is that the corresponding model does not support it. \
-            You can implement you own model rather than use the default model. For more information, please raise an \
-            issue in GitHub repo and we will continue to follow up.
-
-        .. note::
-            For more detailed examples, please refer to our unittest for PDQNPolicy: ``ding.policy.tests.test_pdqn``.
-        """
-        data = default_preprocess_learn(
-            data,
-            use_priority=self._priority,
-            use_priority_IS_weight=self._cfg.priority_IS_weight,
-            ignore_done=self._cfg.learn.ignore_done,
-            use_nstep=True
-        )
-        if self._cuda:
-            data = to_device(data, self._device)
-
-        self.train_cnt += 1
-        # ================================
-        # Continuous args network forward
-        # ================================
-        if self.train_cnt == 1 or self.train_cnt % self._cfg.learn.update_circle in range(5, 10):
-            dis_loss = torch.Tensor([0])
-            td_error_per_sample = torch.Tensor([0])
-            target_q_value = torch.Tensor([0])
-
-            action_args = self._learn_model.forward(data['obs'], mode='compute_continuous')['action_args']
-
-            # Current q value (main model) for cont loss
-            discrete_inputs = {'state': data['obs'], 'action_args': action_args}
-            # with torch.no_grad():
-            q_pi_action_value = self._learn_model.forward(discrete_inputs, mode='compute_discrete')['logit']
-            cont_loss = -q_pi_action_value.sum(dim=-1).mean()
-
-            # ================================
-            # Continuous args network update
-            # ================================
-            self._cont_optimizer.zero_grad()
-            cont_loss.backward()
-            self._cont_optimizer.step()
-
-        # ====================
-        # Q-learning forward
-        # ====================
-        if self.train_cnt == 1 or self.train_cnt % self._cfg.learn.update_circle in range(0, 5):
-            cont_loss = torch.Tensor([0])
-            q_pi_action_value = torch.Tensor([0])
-            self._learn_model.train()
-            self._target_model.train()
-            # Current q value (main model)
-            discrete_inputs = {'state': data['obs'], 'action_args': data['action']['action_args']}
-            q_data_action_args_value = self._learn_model.forward(discrete_inputs, mode='compute_discrete')['logit']
-
-            # Target q value
-            with torch.no_grad():
-                next_action_args = self._learn_model.forward(data['next_obs'], mode='compute_continuous')['action_args']
-                next_action_args_cp = next_action_args.clone().detach()
-                next_discrete_inputs = {'state': data['next_obs'], 'action_args': next_action_args_cp}
-                target_q_value = self._target_model.forward(next_discrete_inputs, mode='compute_discrete')['logit']
-                # Max q value action (main model)
-                target_q_discrete_action = self._learn_model.forward(
-                    next_discrete_inputs, mode='compute_discrete'
-                )['action']['action_type']
-
-            data_n = q_nstep_td_data(
-                q_data_action_args_value, target_q_value, data['action']['action_type'], target_q_discrete_action,
-                data['reward'], data['done'], data['weight']
-            )
-            value_gamma = data.get('value_gamma')
-            dis_loss, td_error_per_sample = q_nstep_td_error(
-                data_n, self._gamma, nstep=self._nstep, value_gamma=value_gamma
-            )
-
-            # ====================
-            # Q-learning update
-            # ====================
-            self._dis_optimizer.zero_grad()
-            dis_loss.backward()
-            self._dis_optimizer.step()
-
-            # =============
-            # after update
-            # =============
-            self._target_model.update(self._learn_model.state_dict())
-
-        return {
-            'cur_lr': self._dis_optimizer.defaults['lr'],
-            'q_loss': dis_loss.item(),
-            'total_loss': cont_loss.item() + dis_loss.item(),
-            'continuous_loss': cont_loss.item(),
-            'q_value': q_pi_action_value.mean().item(),
-            'priority': td_error_per_sample.abs().tolist(),
-            'reward': data['reward'].mean().item(),
-            'target_q_value': target_q_value.mean().item(),
-        }
-
-    def _state_dict_learn(self) -> Dict[str, Any]:
-        """
-        Overview:
-            Return the state_dict of learn mode, usually including model, target model, discrete part optimizer, and \
-            continuous part optimizer.
-        Returns:
-            - state_dict (:obj:`Dict[str, Any]`): the dict of current policy learn state, for saving and restoring.
-        """
-        return {
-            'model': self._learn_model.state_dict(),
-            'target_model': self._target_model.state_dict(),
-            'dis_optimizer': self._dis_optimizer.state_dict(),
-            'cont_optimizer': self._cont_optimizer.state_dict()
-        }
-
-    def _load_state_dict_learn(self, state_dict: Dict[str, Any]) -> None:
-        """
-        Overview:
-            Load the state_dict variable into policy learn mode.
-        Arguments:
-            - state_dict (:obj:`Dict[str, Any]`): the dict of policy learn state saved before.
-
-        .. tip::
-            If you want to only load some parts of model, you can simply set the ``strict`` argument in \
-            load_state_dict to ``False``, or refer to ``ding.torch_utils.checkpoint_helper`` for more \
-            complicated operation.
-        """
-        self._learn_model.load_state_dict(state_dict['model'])
-        self._target_model.load_state_dict(state_dict['target_model'])
-        self._dis_optimizer.load_state_dict(state_dict['dis_optimizer'])
-        self._cont_optimizer.load_state_dict(state_dict['cont_optimizer'])
-
-    def _init_collect(self) -> None:
-        """
-        Overview:
-            Initialize the collect mode of policy, including related attributes and modules. For PDQN, it contains the \
-            collect_model to balance the exploration and exploitation with epsilon-greedy sample mechanism and \
-            continuous action mechanism, besides, other algorithm-specific arguments such as unroll_len and nstep are \
-            also initialized here.
-            This method will be called in ``__init__`` method if ``collect`` field is in ``enable_field``.
-
-        .. note::
-            If you want to set some spacial member variables in ``_init_collect`` method, you'd better name them \
-            with prefix ``_collect_`` to avoid conflict with other modes, such as ``self._collect_attr1``.
-
-        .. tip::
-            Some variables need to initialize independently in different modes, such as gamma and nstep in PDQN. This \
-            design is for the convenience of parallel execution of different policy modes.
-        """
-        self._unroll_len = self._cfg.collect.unroll_len
-        self._gamma = self._cfg.discount_factor  # necessary for parallel
-        self._nstep = self._cfg.nstep  # necessary for parallel
-        self._collect_model = model_wrap(
-            self._model,
-            wrapper_name='action_noise',
-            noise_type='gauss',
-            noise_kwargs={
-                'mu': 0.0,
-                'sigma': self._cfg.collect.noise_sigma
-            },
-            noise_range=None
-        )
-        self._collect_model = model_wrap(self._collect_model, wrapper_name='hybrid_eps_greedy_multinomial_sample')
-        self._collect_model.reset()
-
-    def _forward_collect(self, data: Dict[int, Any], eps: float) -> Dict[int, Any]:
-        """
-        Overview:
-            Policy forward function of collect mode (collecting training data by interacting with envs). Forward means \
-            that the policy gets some necessary data (mainly observation) from the envs and then returns the output \
-            data, such as the action to interact with the envs. Besides, this policy also needs ``eps`` argument for \
-            exploration, i.e., classic epsilon-greedy exploration strategy.
-        Arguments:
-            - data (:obj:`Dict[int, Any]`): The input data used for policy forward, including at least the obs. The \
-                key of the dict is environment id and the value is the corresponding data of the env.
-            - eps (:obj:`float`): The epsilon value for exploration.
-        Returns:
-            - output (:obj:`Dict[int, Any]`): The output data of policy forward, including at least the action and \
-                other necessary data for learn mode defined in ``self._process_transition`` method. The key of the \
-                dict is the same as the input data, i.e. environment id.
-
-        .. note::
-            The input value can be torch.Tensor or dict/list combinations and current policy supports all of them. \
-            For the data type that not supported, the main reason is that the corresponding model does not support it. \
-            You can implement you own model rather than use the default model. For more information, please raise an \
-            issue in GitHub repo and we will continue to follow up.
-
-        .. note::
-            For more detailed examples, please refer to our unittest for PDQNPolicy: ``ding.policy.tests.test_pdqn``.
-        """
-        data_id = list(data.keys())
-        data = default_collate(list(data.values()))
-        if self._cuda:
-            data = to_device(data, self._device)
-        self._collect_model.eval()
-        with torch.no_grad():
-            action_args = self._collect_model.forward(data, 'compute_continuous', eps=eps)['action_args']
-            inputs = {'state': data, 'action_args': action_args.clone().detach()}
-            output = self._collect_model.forward(inputs, 'compute_discrete', eps=eps)
-        if self._cuda:
-            output = to_device(output, 'cpu')
-        output = default_decollate(output)
-        return {i: d for i, d in zip(data_id, output)}
-
-    def _get_train_sample(self, transitions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
-        """
-        Overview:
-            For a given trajectory (transitions, a list of transition) data, process it into a list of sample that \
-            can be used for training directly. In PDQN, a train sample is a processed transition. \
-            This method is usually used in collectors to execute necessary \
-            RL data preprocessing before training, which can help learner amortize revelant time consumption. \
-            In addition, you can also implement this method as an identity function and do the data processing \
-            in ``self._forward_learn`` method.
-        Arguments:
-            - transitions (:obj:`List[Dict[str, Any]`): The trajectory data (a list of transition), each element is \
-                the same format as the return value of ``self._process_transition`` method.
-        Returns:
-            - samples (:obj:`List[Dict[str, Any]]`): The processed train samples, each element is the similar format \
-                as input transitions, but may contain more data for training, such as nstep reward and target obs.
-        """
-        transitions = get_nstep_return_data(transitions, self._nstep, gamma=self._gamma)
-        return get_train_sample(transitions, self._unroll_len)
-
-    def _process_transition(self, obs: torch.Tensor, policy_output: Dict[str, torch.Tensor],
-                            timestep: namedtuple) -> Dict[str, torch.Tensor]:
-        """
-        Overview:
-            Process and pack one timestep transition data into a dict, which can be directly used for training and \
-            saved in replay buffer. For PDQN, it contains obs, next_obs, action, reward, done and logit.
-        Arguments:
-            - obs (:obj:`torch.Tensor`): The env observation of current timestep, such as stacked 2D image in Atari.
-            - policy_output (:obj:`Dict[str, torch.Tensor]`): The output of the policy network with the observation \
-                as input. For PDQN, it contains the hybrid action and the logit (discrete part q_value) of the action.
-            - timestep (:obj:`namedtuple`): The execution result namedtuple returned by the environment step method, \
-                except all the elements have been transformed into tensor data. Usually, it contains the next obs, \
-                reward, done, info, etc.
-        Returns:
-            - transition (:obj:`Dict[str, torch.Tensor]`): The processed transition data of the current timestep.
-        """
-        transition = {
-            'obs': obs,
-            'next_obs': timestep.obs,
-            'action': policy_output['action'],
-            'logit': policy_output['logit'],
-            'reward': timestep.reward,
-            'done': timestep.done,
-        }
-        return transition
-
-    def _init_eval(self) -> None:
-        """
-        Overview:
-            Initialize the eval mode of policy, including related attributes and modules. For PDQN, it contains the \
-            eval model to greedily select action with argmax q_value mechanism.
-            This method will be called in ``__init__`` method if ``eval`` field is in ``enable_field``.
-
-        .. note::
-            If you want to set some spacial member variables in ``_init_eval`` method, you'd better name them \
-            with prefix ``_eval_`` to avoid conflict with other modes, such as ``self._eval_attr1``.
-        """
-        self._eval_model = model_wrap(self._model, wrapper_name='hybrid_argmax_sample')
-        self._eval_model.reset()
-
-    def _forward_eval(self, data: Dict[int, Any]) -> Dict[int, Any]:
-        """
-        Overview:
-            Policy forward function of eval mode (evaluation policy performance by interacting with envs). Forward \
-            means that the policy gets some necessary data (mainly observation) from the envs and then returns the \
-            action to interact with the envs.
-        Arguments:
-            - data (:obj:`Dict[int, Any]`): The input data used for policy forward, including at least the obs. The \
-                key of the dict is environment id and the value is the corresponding data of the env.
-        Returns:
-            - output (:obj:`Dict[int, Any]`): The output data of policy forward, including at least the action. The \
-                key of the dict is the same as the input data, i.e. environment id.
-
-        .. note::
-            The input value can be torch.Tensor or dict/list combinations and current policy supports all of them. \
-            For the data type that not supported, the main reason is that the corresponding model does not support it. \
-            You can implement you own model rather than use the default model. For more information, please raise an \
-            issue in GitHub repo and we will continue to follow up.
-
-        .. note::
-            For more detailed examples, please refer to our unittest for PDQNPolicy: ``ding.policy.tests.test_pdqn``.
-        """
-        data_id = list(data.keys())
-        data = default_collate(list(data.values()))
-        if self._cuda:
-            data = to_device(data, self._device)
-        self._eval_model.eval()
-        with torch.no_grad():
-            action_args = self._eval_model.forward(data, mode='compute_continuous')['action_args']
-            inputs = {'state': data, 'action_args': action_args.clone().detach()}
-            output = self._eval_model.forward(inputs, mode='compute_discrete')
-        if self._cuda:
-            output = to_device(output, 'cpu')
-        output = default_decollate(output)
-        return {i: d for i, d in zip(data_id, output)}
-
-    def _monitor_vars_learn(self) -> List[str]:
-        """
-        Overview:
-            Return the necessary keys for logging the return dict of ``self._forward_learn``. The logger module, such \
-            as text logger, tensorboard logger, will use these keys to save the corresponding data.
-        Returns:
-            - necessary_keys (:obj:`List[str]`): The list of the necessary keys to be logged.
-        """
-        return ['cur_lr', 'total_loss', 'q_loss', 'continuous_loss', 'q_value', 'reward', 'target_q_value']
diff --git a/requirements.txt b/requirements.txt
index 287db91..0b7a1d7 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -2,6 +2,7 @@ absl-py==2.3.0
 annotated-types==0.7.0
 astroid==3.3.10
 asttokens==3.0.0
+astunparse==1.6.3
 async-timeout==5.0.1
 attrs==25.3.0
 bitmath==1.3.3.1
@@ -40,6 +41,7 @@ filelock==3.18.0
 flake8==7.2.0
 Flask==2.0.3
 flask-cors==6.0.0
+flatbuffers==25.2.10
 flax==0.10.6
 fonttools==4.58.0
 fsspec==2025.5.1
@@ -47,6 +49,7 @@ gast==0.6.0
 gitdb==4.0.12
 GitPython==3.1.44
 glfw==2.9.0
+google-pasta==0.2.0
 graphviz==0.20.3
 grpcio==1.71.0
 gym==0.25.1
@@ -76,7 +79,9 @@ Jinja2==3.1.6
 joblib==1.5.1
 jupyter_client==8.6.3
 jupyter_core==5.8.1
+keras==3.10.0
 kiwisolver==1.4.8
+libclang==18.1.1
 llvmlite==0.44.0
 Markdown==3.8
 markdown-it-py==3.0.0
@@ -93,6 +98,7 @@ msgpack==1.1.0
 mujoco==3.3.2
 mujoco-mjx==3.3.2
 mypy_extensions==1.1.0
+namex==0.1.0
 Navix==0.7.0
 nest-asyncio==1.6.0
 networkx==3.4.2
@@ -115,6 +121,7 @@ nvidia-nvshmem-cu12==3.2.5
 nvidia-nvtx-cu12==12.6.77
 opt_einsum==3.4.0
 optax==0.2.4
+optree==0.16.0
 orbax-checkpoint==0.11.13
 packaging==25.0
 pandas==2.2.3
@@ -172,8 +179,11 @@ tabulate==0.9.0
 tensorboard==2.19.0
 tensorboard-data-server==0.7.2
 tensorboardX==2.6.2.2
+tensorflow==2.19.0
+tensorflow-io-gcs-filesystem==0.37.1
 tensorflow-probability==0.25.0
 tensorstore==0.1.75
+termcolor==3.1.0
 threadpoolctl==3.6.0
 tokenizers==0.21.1
 tomli==2.2.1
diff --git a/template.pdqn.py b/template.pdqn.py
deleted file mode 100644
index ec94cb3..0000000
--- a/template.pdqn.py
+++ /dev/null
@@ -1,229 +0,0 @@
-from typing import Union, Optional, Dict
-from easydict import EasyDict
-
-import torch
-import torch.nn as nn
-
-from ding.torch_utils import get_lstm
-from ding.utils import MODEL_REGISTRY, SequenceType, squeeze
-from ..common import FCEncoder, ConvEncoder, DiscreteHead, DuelingHead, RegressionHead
-
-
-@MODEL_REGISTRY.register('pdqn')
-class PDQN(nn.Module):
-    """
-    Overview:
-        The neural network and computation graph of PDQN(https://arxiv.org/abs/1810.06394v1) and \
-        MPDQN(https://arxiv.org/abs/1905.04388) algorithms for parameterized action space. \
-        This model supports parameterized action space with discrete ``action_type`` and continuous ``action_arg``. \
-        In principle, PDQN consists of x network (continuous action parameter network) and Q network (discrete \
-        action type network). But for simplicity, the code is split into ``encoder`` and ``actor_head``, which \
-        contain the encoder and head of the above two networks respectively.
-    Interface:
-        ``__init__``, ``forward``, ``compute_discrete``, ``compute_continuous``.
-    """
-    mode = ['compute_discrete', 'compute_continuous']
-
-    def __init__(
-            self,
-            obs_shape: Union[int, SequenceType],
-            action_shape: EasyDict,
-            encoder_hidden_size_list: SequenceType = [128, 128, 64],
-            dueling: bool = True,
-            head_hidden_size: Optional[int] = None,
-            head_layer_num: int = 1,
-            activation: Optional[nn.Module] = nn.ReLU(),
-            norm_type: Optional[str] = None,
-            multi_pass: Optional[bool] = False,
-            action_mask: Optional[list] = None
-    ) -> None:
-        """
-        Overview:
-            Init the PDQN (encoder + head) Model according to input arguments.
-        Arguments:
-            - obs_shape (:obj:`Union[int, SequenceType]`): Observation space shape, such as 8 or [4, 84, 84].
-            - action_shape (:obj:`EasyDict`): Action space shape in dict type, such as \
-                EasyDict({'action_type_shape': 3, 'action_args_shape': 5}).
-            - encoder_hidden_size_list (:obj:`SequenceType`): Collection of ``hidden_size`` to pass to ``Encoder``, \
-                the last element must match ``head_hidden_size``.
-            - dueling (:obj:`dueling`): Whether choose ``DuelingHead`` or ``DiscreteHead(default)``.
-            - head_hidden_size (:obj:`Optional[int]`): The ``hidden_size`` of head network.
-            - head_layer_num (:obj:`int`): The number of layers used in the head network to compute Q value output.
-            - activation (:obj:`Optional[nn.Module]`): The type of activation function in networks \
-                if ``None`` then default set it to ``nn.ReLU()``.
-            - norm_type (:obj:`Optional[str]`): The type of normalization in networks, see \
-                ``ding.torch_utils.fc_block`` for more details.
-            - multi_pass (:obj:`Optional[bool]`): Whether to use multi pass version.
-            - action_mask: (:obj:`Optional[list]`): An action mask indicating how action args are \
-                associated to each discrete action. For example, if there are 3 discrete action, \
-                4 continous action args, and the first discrete action associates with the first \
-                continuous action args, the second discrete action associates with the second continuous \
-                action args, and the third discrete action associates with the remaining 2 action args, \
-                the action mask will be like: [[1,0,0,0],[0,1,0,0],[0,0,1,1]] with shape 3*4.
-        """
-        super(PDQN, self).__init__()
-        self.multi_pass = multi_pass
-        if self.multi_pass:
-            assert isinstance(
-                action_mask, list
-            ), 'Please indicate action mask in list form if you set multi_pass to True'
-            self.action_mask = torch.LongTensor(action_mask)
-            nonzero = torch.nonzero(self.action_mask)
-            index = torch.zeros(action_shape.action_args_shape).long()
-            index.scatter_(dim=0, index=nonzero[:, 1], src=nonzero[:, 0])
-            self.action_scatter_index = index  # (self.action_args_shape, )
-
-        # squeeze action shape input like (3,) to 3
-        action_shape.action_args_shape = squeeze(action_shape.action_args_shape)
-        action_shape.action_type_shape = squeeze(action_shape.action_type_shape)
-        self.action_args_shape = action_shape.action_args_shape
-        self.action_type_shape = action_shape.action_type_shape
-
-        # init head hidden size
-        if head_hidden_size is None:
-            head_hidden_size = encoder_hidden_size_list[-1]
-
-        # squeeze obs input for compatibility: 1, (1, ), [4, 32, 32]
-        obs_shape = squeeze(obs_shape)
-
-        # Obs Encoder Type
-        if isinstance(obs_shape, int) or len(obs_shape) == 1:  # FC Encoder
-            self.dis_encoder = FCEncoder(
-                obs_shape, encoder_hidden_size_list, activation=activation, norm_type=norm_type
-            )
-            self.cont_encoder = FCEncoder(
-                obs_shape, encoder_hidden_size_list, activation=activation, norm_type=norm_type
-            )
-        elif len(obs_shape) == 3:  # Conv Encoder
-            self.dis_encoder = ConvEncoder(
-                obs_shape, encoder_hidden_size_list, activation=activation, norm_type=norm_type
-            )
-            self.cont_encoder = ConvEncoder(
-                obs_shape, encoder_hidden_size_list, activation=activation, norm_type=norm_type
-            )
-        else:
-            raise RuntimeError(
-                "Pre-defined encoder not support obs_shape {}, please customize your own PDQN.".format(obs_shape)
-            )
-
-        # Continuous Action Head Type
-        self.cont_head = RegressionHead(
-            head_hidden_size,
-            action_shape.action_args_shape,
-            head_layer_num,
-            final_tanh=True,
-            activation=activation,
-            norm_type=norm_type
-        )
-
-        # Discrete Action Head Type
-        if dueling:
-            dis_head_cls = DuelingHead
-        else:
-            dis_head_cls = DiscreteHead
-        self.dis_head = dis_head_cls(
-            head_hidden_size + action_shape.action_args_shape,
-            action_shape.action_type_shape,
-            head_layer_num,
-            activation=activation,
-            norm_type=norm_type
-        )
-
-        self.actor_head = nn.ModuleList([self.dis_head, self.cont_head])
-        # self.encoder = nn.ModuleList([self.dis_encoder, self.cont_encoder])
-        # To speed up the training process, the X network and the Q network share the encoder for the state
-        self.encoder = nn.ModuleList([self.cont_encoder, self.cont_encoder])
-
-    def forward(self, inputs: Union[torch.Tensor, Dict, EasyDict], mode: str) -> Dict:
-        """
-        Overview:
-            PDQN forward computation graph, input observation tensor to predict q_value for \
-            discrete actions and values for continuous action_args.
-        Arguments:
-            - inputs (:obj:`Union[torch.Tensor, Dict, EasyDict]`): Inputs including observation and \
-                other info according to `mode`.
-            - mode (:obj:`str`): Name of the forward mode.
-        Shapes:
-            - inputs (:obj:`torch.Tensor`): :math:`(B, N)`, where B is batch size and N is ``obs_shape``.
-        """
-        assert mode in self.mode, "not support forward mode: {}/{}".format(mode, self.mode)
-        return getattr(self, mode)(inputs)
-
-    def compute_continuous(self, inputs: torch.Tensor) -> Dict:
-        """
-        Overview:
-            Use observation tensor to predict continuous action args.
-        Arguments:
-            - inputs (:obj:`torch.Tensor`): Observation inputs.
-        Returns:
-            - outputs (:obj:`Dict`): A dict with key 'action_args'.
-                - 'action_args' (:obj:`torch.Tensor`): The continuous action args.
-        Shapes:
-            - inputs (:obj:`torch.Tensor`): :math:`(B, N)`, where B is batch size and N is ``obs_shape``.
-            - action_args (:obj:`torch.Tensor`): :math:`(B, M)`, where M is ``action_args_shape``.
-        Examples:
-            >>> act_shape = EasyDict({'action_type_shape': (3, ), 'action_args_shape': (5, )})
-            >>> model = PDQN(4, act_shape)
-            >>> inputs = torch.randn(64, 4)
-            >>> outputs = model.forward(inputs, mode='compute_continuous')
-            >>> assert outputs['action_args'].shape == torch.Size([64, 5])
-        """
-        cont_x = self.encoder[1](inputs)  # size (B, encoded_state_shape)
-        action_args = self.actor_head[1](cont_x)['pred']  # size (B, action_args_shape)
-        outputs = {'action_args': action_args}
-        return outputs
-
-    def compute_discrete(self, inputs: Union[Dict, EasyDict]) -> Dict:
-        """
-        Overview:
-            Use observation tensor and continuous action args to predict discrete action types.
-        Arguments:
-            - inputs (:obj:`Union[Dict, EasyDict]`): A dict with keys 'state', 'action_args'.
-                - state (:obj:`torch.Tensor`): Observation inputs.
-                - action_args (:obj:`torch.Tensor`): Action parameters are used to concatenate with the observation \
-                    and serve as input to the discrete action type network.
-        Returns:
-            - outputs (:obj:`Dict`): A dict with keys 'logit', 'action_args'.
-                -  'logit': The logit value for each discrete action.
-                -  'action_args': The continuous action args(same as the inputs['action_args']) for later usage.
-        Examples:
-            >>> act_shape = EasyDict({'action_type_shape': (3, ), 'action_args_shape': (5, )})
-            >>> model = PDQN(4, act_shape)
-            >>> inputs = {'state': torch.randn(64, 4), 'action_args': torch.randn(64, 5)}
-            >>> outputs = model.forward(inputs, mode='compute_discrete')
-            >>> assert outputs['logit'].shape == torch.Size([64, 3])
-            >>> assert outputs['action_args'].shape == torch.Size([64, 5])
-        """
-        dis_x = self.encoder[0](inputs['state'])  # size (B, encoded_state_shape)
-        action_args = inputs['action_args']  # size (B, action_args_shape)
-
-        if self.multi_pass:  # mpdqn
-            # fill_value=-2 is a mask value, which is not in normal acton range
-            # (B, action_args_shape, K) where K is the action_type_shape
-            mp_action = torch.full(
-                (dis_x.shape[0], self.action_args_shape, self.action_type_shape),
-                fill_value=-2,
-                device=dis_x.device,
-                dtype=dis_x.dtype
-            )
-            index = self.action_scatter_index.view(1, -1, 1).repeat(dis_x.shape[0], 1, 1).to(dis_x.device)
-
-            # index: (B, action_args_shape, 1)  src: (B, action_args_shape, 1)
-            mp_action.scatter_(dim=-1, index=index, src=action_args.unsqueeze(-1))
-            mp_action = mp_action.permute(0, 2, 1)  # (B, K, action_args_shape)
-
-            mp_state = dis_x.unsqueeze(1).repeat(1, self.action_type_shape, 1)  # (B, K, obs_shape)
-            mp_state_action_cat = torch.cat([mp_state, mp_action], dim=-1)
-
-            logit = self.actor_head[0](mp_state_action_cat)['logit']  # (B, K, K)
-
-            logit = torch.diagonal(logit, dim1=-2, dim2=-1)  # (B, K)
-        else:  # pdqn
-            # size (B, encoded_state_shape + action_args_shape)
-            if len(action_args.shape) == 1:  # (B, ) -> (B, 1)
-                action_args = action_args.unsqueeze(1)
-            state_action_cat = torch.cat((dis_x, action_args), dim=-1)
-            logit = self.actor_head[0](state_action_cat)['logit']  # size (B, K) where K is action_type_shape
-
-        outputs = {'logit': logit, 'action_args': action_args}
-        return outputs