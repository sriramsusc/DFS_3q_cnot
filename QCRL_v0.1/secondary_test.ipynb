{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b1edbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fidelity(A: np.ndarray, B: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute  F = (1 / (n (n + 1))) * ( Tr(A†A) + |Tr(B†A)|² )\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A, B : np.ndarray\n",
    "        n × n complex or real matrices.  Must have identical shape.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The fidelity value.\n",
    "    \"\"\"\n",
    "    # --- basic validation ----------------------------------------------------\n",
    "    if A.shape != B.shape or A.ndim != 2 or A.shape[0] != A.shape[1]:\n",
    "        raise ValueError(\"A and B must be square matrices of the same size\")\n",
    "\n",
    "    n = A.shape[0]\n",
    "\n",
    "    # --- core computation ----------------------------------------------------\n",
    "    term1 = np.trace(A.conj().T @ A)                 # Tr(A†A)\n",
    "    term2 = np.trace(B.conj().T @ A)                 # Tr(B†A)\n",
    "    fidelity_val = (term1 + abs(term2)**2) / (n * (n + 1))\n",
    "\n",
    "    # If you prefer a plain Python float:\n",
    "    return float(np.real_if_close(fidelity_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8be15ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1953b265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frobenius_diff(A: np.ndarray, B: np.ndarray) -> float:\n",
    "    diff = A - B\n",
    "    # Using numpy.linalg.norm with 'fro' gives exactly the definition above.\n",
    "    return np.linalg.norm(A-B, ord='fro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "457de8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import chex\n",
    "import numpy as np\n",
    "from flax import struct\n",
    "from functools import partial\n",
    "from typing import Optional, Tuple, Union, Any\n",
    "from gymnax.environments import environment, spaces\n",
    "from brax import envs\n",
    "from brax.envs.wrappers.training import EpisodeWrapper, AutoResetWrapper\n",
    "import navix as nx\n",
    "\n",
    "\n",
    "class GymnaxWrapper(object):\n",
    "    \"\"\"Base class for Gymnax wrappers.\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self._env = env\n",
    "\n",
    "    # provide proxy access to regular attributes of wrapped object\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self._env, name)\n",
    "\n",
    "\n",
    "class FlattenObservationWrapper(GymnaxWrapper):\n",
    "    \"\"\"Flatten the observations of the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, env: environment.Environment):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def observation_space(self, params) -> spaces.Box:\n",
    "        assert isinstance(\n",
    "            self._env.observation_space(params), spaces.Box\n",
    "        ), \"Only Box spaces are supported for now.\"\n",
    "        return spaces.Box(\n",
    "            low=self._env.observation_space(params).low,\n",
    "            high=self._env.observation_space(params).high,\n",
    "            shape=(np.prod(self._env.observation_space(params).shape),),\n",
    "            dtype=self._env.observation_space(params).dtype,\n",
    "        )\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def reset(\n",
    "        self, key: chex.PRNGKey, params: Optional[environment.EnvParams] = None\n",
    "    ) -> Tuple[chex.Array, environment.EnvState]:\n",
    "        obs, state = self._env.reset(key, params)\n",
    "        obs = jnp.reshape(obs, (-1,))\n",
    "        return obs, state\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def step(\n",
    "        self,\n",
    "        key: chex.PRNGKey,\n",
    "        state: environment.EnvState,\n",
    "        action: Union[int, float],\n",
    "        params: Optional[environment.EnvParams] = None,\n",
    "    ) -> Tuple[chex.Array, environment.EnvState, float, bool, dict]:\n",
    "        obs, state, reward, done, info = self._env.step(key, state, action, params)\n",
    "        obs = jnp.reshape(obs, (-1,))\n",
    "        return obs, state, reward, done, info\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class LogEnvState:\n",
    "    env_state: environment.EnvState\n",
    "    episode_returns: float\n",
    "    episode_lengths: int\n",
    "    returned_episode_returns: float\n",
    "    returned_episode_lengths: int\n",
    "    timestep: int\n",
    "\n",
    "\n",
    "class LogWrapper(GymnaxWrapper):\n",
    "    \"\"\"Log the episode returns and lengths.\"\"\"\n",
    "\n",
    "    def __init__(self, env: environment.Environment):\n",
    "        super().__init__(env)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def reset(\n",
    "        self, key: chex.PRNGKey, params: Optional[environment.EnvParams] = None\n",
    "    ) -> Tuple[chex.Array, environment.EnvState]:\n",
    "        obs, env_state = self._env.reset(key, params)\n",
    "        state = LogEnvState(env_state, 0, 0, 0, 0, 0)\n",
    "        return obs, state\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def step(\n",
    "        self,\n",
    "        key: chex.PRNGKey,\n",
    "        state: environment.EnvState,\n",
    "        action: Union[int, float],\n",
    "        params: Optional[environment.EnvParams] = None,\n",
    "    ) -> Tuple[chex.Array, environment.EnvState, float, bool, dict]:\n",
    "        obs, env_state, reward, done, info = self._env.step(\n",
    "            key, state.env_state, action, params\n",
    "        )\n",
    "        new_episode_return = state.episode_returns + reward\n",
    "        new_episode_length = state.episode_lengths + 1\n",
    "        state = LogEnvState(\n",
    "            env_state=env_state,\n",
    "            episode_returns=new_episode_return * (1 - done),\n",
    "            episode_lengths=new_episode_length * (1 - done),\n",
    "            returned_episode_returns=state.returned_episode_returns * (1 - done)\n",
    "            + new_episode_return * done,\n",
    "            returned_episode_lengths=state.returned_episode_lengths * (1 - done)\n",
    "            + new_episode_length * done,\n",
    "            timestep=state.timestep + 1,\n",
    "        )\n",
    "        info[\"returned_episode_returns\"] = state.returned_episode_returns\n",
    "        info[\"returned_episode_lengths\"] = state.returned_episode_lengths\n",
    "        info[\"timestep\"] = state.timestep\n",
    "        info[\"returned_episode\"] = done\n",
    "        return obs, state, reward, done, info\n",
    "\n",
    "\n",
    "class BraxGymnaxWrapper:\n",
    "    def __init__(self, env_name, backend=\"positional\"):\n",
    "        env = envs.get_environment(env_name=env_name, backend=backend)\n",
    "        env = EpisodeWrapper(env, episode_length=1000, action_repeat=1)\n",
    "        env = AutoResetWrapper(env)\n",
    "        self._env = env\n",
    "        self.action_size = env.action_size\n",
    "        self.observation_size = (env.observation_size,)\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        state = self._env.reset(key)\n",
    "        return state.obs, state\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        next_state = self._env.step(state, action)\n",
    "        return next_state.obs, next_state, next_state.reward, next_state.done > 0.5, {}\n",
    "\n",
    "    def observation_space(self, params):\n",
    "        return spaces.Box(\n",
    "            low=-jnp.inf,\n",
    "            high=jnp.inf,\n",
    "            shape=(self._env.observation_size,),\n",
    "        )\n",
    "\n",
    "    def action_space(self, params):\n",
    "        return spaces.Box(\n",
    "            low=-1.0,\n",
    "            high=1.0,\n",
    "            shape=(self._env.action_size,),\n",
    "        )\n",
    "\n",
    "class NavixGymnaxWrapper:\n",
    "    def __init__(self, env_name):\n",
    "        self._env = nx.make(env_name)\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        timestep = self._env.reset(key)\n",
    "        return timestep.observation, timestep\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        timestep = self._env.step(state, action)\n",
    "        return timestep.observation, timestep, timestep.reward, timestep.is_done(), {}\n",
    "\n",
    "    def observation_space(self, params):\n",
    "        return spaces.Box(\n",
    "            low=self._env.observation_space.minimum,\n",
    "            high=self._env.observation_space.maximum,\n",
    "            shape=(np.prod(self._env.observation_space.shape),),\n",
    "            dtype=self._env.observation_space.dtype,\n",
    "        )\n",
    "\n",
    "    def action_space(self, params):\n",
    "        return spaces.Discrete(\n",
    "            num_categories=self._env.action_space.maximum.item() + 1,\n",
    "        )\n",
    "\n",
    "\n",
    "class ClipAction(GymnaxWrapper):\n",
    "    def __init__(self, env, low=-1.0, high=1.0):\n",
    "        super().__init__(env)\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        \"\"\"TODO: In theory the below line should be the way to do this.\"\"\"\n",
    "        # action = jnp.clip(action, self.env.action_space.low, self.env.action_space.high)\n",
    "        action = jnp.clip(action, self.low, self.high)\n",
    "        return self._env.step(key, state, action, params)\n",
    "\n",
    "\n",
    "class TransformObservation(GymnaxWrapper):\n",
    "    def __init__(self, env, transform_obs):\n",
    "        super().__init__(env)\n",
    "        self.transform_obs = transform_obs\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        obs, state = self._env.reset(key, params)\n",
    "        return self.transform_obs(obs), state\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        obs, state, reward, done, info = self._env.step(key, state, action, params)\n",
    "        return self.transform_obs(obs), state, reward, done, info\n",
    "\n",
    "\n",
    "class TransformReward(GymnaxWrapper):\n",
    "    def __init__(self, env, transform_reward):\n",
    "        super().__init__(env)\n",
    "        self.transform_reward = transform_reward\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        obs, state, reward, done, info = self._env.step(key, state, action, params)\n",
    "        return obs, state, self.transform_reward(reward), done, info\n",
    "\n",
    "\n",
    "class VecEnv(GymnaxWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.reset = jax.vmap(self._env.reset, in_axes=(0, None))\n",
    "        self.step = jax.vmap(self._env.step, in_axes=(0, 0, 0, None))\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class NormalizeVecObsEnvState:\n",
    "    mean: jnp.ndarray\n",
    "    var: jnp.ndarray\n",
    "    count: float\n",
    "    env_state: environment.EnvState\n",
    "\n",
    "\n",
    "class NormalizeVecObservation(GymnaxWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        obs, state = self._env.reset(key, params)\n",
    "        state = NormalizeVecObsEnvState(\n",
    "            mean=jnp.zeros_like(obs),\n",
    "            var=jnp.ones_like(obs),\n",
    "            count=1e-4,\n",
    "            env_state=state,\n",
    "        )\n",
    "        batch_mean = jnp.mean(obs, axis=0)\n",
    "        batch_var = jnp.var(obs, axis=0)\n",
    "        batch_count = obs.shape[0]\n",
    "\n",
    "        delta = batch_mean - state.mean\n",
    "        tot_count = state.count + batch_count\n",
    "\n",
    "        new_mean = state.mean + delta * batch_count / tot_count\n",
    "        m_a = state.var * state.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + jnp.square(delta) * state.count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        new_count = tot_count\n",
    "\n",
    "        state = NormalizeVecObsEnvState(\n",
    "            mean=new_mean,\n",
    "            var=new_var,\n",
    "            count=new_count,\n",
    "            env_state=state.env_state,\n",
    "        )\n",
    "\n",
    "        return (obs - state.mean) / jnp.sqrt(state.var + 1e-8), state\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        obs, env_state, reward, done, info = self._env.step(\n",
    "            key, state.env_state, action, params\n",
    "        )\n",
    "\n",
    "        batch_mean = jnp.mean(obs, axis=0)\n",
    "        batch_var = jnp.var(obs, axis=0)\n",
    "        batch_count = obs.shape[0]\n",
    "\n",
    "        delta = batch_mean - state.mean\n",
    "        tot_count = state.count + batch_count\n",
    "\n",
    "        new_mean = state.mean + delta * batch_count / tot_count\n",
    "        m_a = state.var * state.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + jnp.square(delta) * state.count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        new_count = tot_count\n",
    "\n",
    "        state = NormalizeVecObsEnvState(\n",
    "            mean=new_mean,\n",
    "            var=new_var,\n",
    "            count=new_count,\n",
    "            env_state=env_state,\n",
    "        )\n",
    "        return (\n",
    "            (obs - state.mean) / jnp.sqrt(state.var + 1e-8),\n",
    "            state,\n",
    "            reward,\n",
    "            done,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class NormalizeVecRewEnvState:\n",
    "    mean: jnp.ndarray\n",
    "    var: jnp.ndarray\n",
    "    count: float\n",
    "    return_val: float\n",
    "    env_state: environment.EnvState\n",
    "\n",
    "\n",
    "class NormalizeVecReward(GymnaxWrapper):\n",
    "    def __init__(self, env, gamma):\n",
    "        super().__init__(env)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        obs, state = self._env.reset(key, params)\n",
    "        batch_count = obs.shape[0]\n",
    "        state = NormalizeVecRewEnvState(\n",
    "            mean=0.0,\n",
    "            var=1.0,\n",
    "            count=1e-4,\n",
    "            return_val=jnp.zeros((batch_count,)),\n",
    "            env_state=state,\n",
    "        )\n",
    "        return obs, state\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        obs, env_state, reward, done, info = self._env.step(\n",
    "            key, state.env_state, action, params\n",
    "        )\n",
    "        return_val = state.return_val * self.gamma * (1 - done) + reward\n",
    "\n",
    "        batch_mean = jnp.mean(return_val, axis=0)\n",
    "        batch_var = jnp.var(return_val, axis=0)\n",
    "        batch_count = obs.shape[0]\n",
    "\n",
    "        delta = batch_mean - state.mean\n",
    "        tot_count = state.count + batch_count\n",
    "\n",
    "        new_mean = state.mean + delta * batch_count / tot_count\n",
    "        m_a = state.var * state.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + jnp.square(delta) * state.count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        new_count = tot_count\n",
    "\n",
    "        state = NormalizeVecRewEnvState(\n",
    "            mean=new_mean,\n",
    "            var=new_var,\n",
    "            count=new_count,\n",
    "            return_val=return_val,\n",
    "            env_state=env_state,\n",
    "        )\n",
    "        return obs, state, reward / jnp.sqrt(state.var + 1e-8), done, info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
